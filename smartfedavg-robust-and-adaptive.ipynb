{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-14T05:06:40.988500Z","iopub.execute_input":"2025-06-14T05:06:40.988789Z","iopub.status.idle":"2025-06-14T05:06:41.941889Z","shell.execute_reply.started":"2025-06-14T05:06:40.988757Z","shell.execute_reply":"2025-06-14T05:06:41.941003Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset, Dataset\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport copy\nimport time\nimport json\nimport logging\nimport pandas as pd\nfrom datetime import datetime\nfrom pathlib import Path\nfrom collections import defaultdict\nimport warnings\nimport cv2\nfrom PIL import Image\nfrom itertools import product\nwarnings.filterwarnings('ignore')\n\n# Set style for better plots\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\ndef safe_aggregate_models(global_dict, client_models, weights):\n    \"\"\"Safely aggregate models handling different parameter types\"\"\"\n    aggregated_dict = {}\n    \n    for key in global_dict.keys():\n        if global_dict[key].dtype in [torch.long, torch.int, torch.int32, torch.int64]:\n            aggregated_dict[key] = client_models[0].state_dict()[key].clone()\n        else:\n            aggregated_dict[key] = torch.zeros_like(global_dict[key], dtype=torch.float32)\n            for model, weight in zip(client_models, weights):\n                model_dict = model.state_dict()\n                aggregated_dict[key] += weight * model_dict[key].float()\n            if global_dict[key].dtype != torch.float32:\n                aggregated_dict[key] = aggregated_dict[key].to(global_dict[key].dtype)\n    \n    return aggregated_dict\n\nclass ParameterSweepConfig:\n    \"\"\"Configuration for comprehensive parameter sweep\"\"\"\n    def __init__(self, lr, alpha, quality_level, run_id=1):\n        # Basic FL settings\n        self.num_clients = 10\n        self.clients_per_round = 6\n        self.num_rounds = 8\n        self.local_epochs = 3\n        self.batch_size = 32\n        self.lr = lr\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        # Non-IID parameter\n        self.alpha_dirichlet = alpha\n        self.min_samples_per_client = 200\n        \n        # Quality degradation level\n        self.quality_level = quality_level\n        self.enable_quality_degradation = quality_level != 'low'\n        \n        # Quality ratios based on degradation level\n        if quality_level == 'low':\n            self.high_quality_ratio = 1.0\n            self.medium_quality_ratio = 0.0\n            self.low_quality_ratio = 0.0\n        elif quality_level == 'medium':\n            self.high_quality_ratio = 0.5\n            self.medium_quality_ratio = 0.3\n            self.low_quality_ratio = 0.2\n        else:  # 'high' degradation\n            self.high_quality_ratio = 0.2\n            self.medium_quality_ratio = 0.3\n            self.low_quality_ratio = 0.5\n        \n        # Experiment settings\n        self.num_runs = 1\n        self.seed_base = 42 + run_id * 1000\n        \n        # Experiment identifier\n        self.experiment_id = f\"lr{lr}_alpha{alpha}_quality{quality_level}_{run_id}\"\n\nclass CIFAR10Model(nn.Module):\n    \"\"\"Optimized CNN for CIFAR-10\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n        self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n        \n        self.bn1 = nn.BatchNorm2d(64)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.bn4 = nn.BatchNorm2d(128)\n        self.bn5 = nn.BatchNorm2d(256)\n        \n        self.pool = nn.MaxPool2d(2, 2)\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((2, 2))\n        \n        self.fc1 = nn.Linear(256 * 2 * 2, 512)\n        self.fc2 = nn.Linear(512, 128)\n        self.fc3 = nn.Linear(128, 10)\n        self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.pool(x)\n        \n        x = F.relu(self.bn3(self.conv3(x)))\n        x = F.relu(self.bn4(self.conv4(x)))\n        x = self.pool(x)\n        \n        x = F.relu(self.bn5(self.conv5(x)))\n        x = self.pool(x)\n        \n        x = self.adaptive_pool(x)\n        x = x.view(x.size(0), -1)\n        \n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.fc3(x)\n        return x\n\nclass QualityDegradedDataset(Dataset):\n    \"\"\"Dataset wrapper that applies quality degradations\"\"\"\n    def __init__(self, base_dataset, quality_level='low', corruption_seed=42):\n        self.base_dataset = base_dataset\n        self.quality_level = quality_level\n        \n        np.random.seed(corruption_seed)\n        torch.manual_seed(corruption_seed)\n        \n        # Define corruption parameters\n        if quality_level == 'low':  # Low degradation (high quality)\n            self.label_noise_rate = 0.01\n            self.feature_noise_std = 0.005\n            self.blur_prob = 0.02\n            self.contrast_factor = 0.99\n        elif quality_level == 'medium':  # Medium degradation\n            self.label_noise_rate = 0.08\n            self.feature_noise_std = 0.04\n            self.blur_prob = 0.15\n            self.contrast_factor = 0.85\n        else:  # 'high' degradation (low quality)\n            self.label_noise_rate = 0.20\n            self.feature_noise_std = 0.12\n            self.blur_prob = 0.30\n            self.contrast_factor = 0.70\n        \n        self.corrupted_labels = {}\n        self._generate_label_corruptions()\n    \n    def _generate_label_corruptions(self):\n        if self.label_noise_rate > 0:\n            num_samples = len(self.base_dataset)\n            num_corrupt = int(num_samples * self.label_noise_rate)\n            corrupt_indices = np.random.choice(num_samples, num_corrupt, replace=False)\n            \n            for idx in corrupt_indices:\n                original_label = self.base_dataset[idx][1]\n                wrong_labels = [i for i in range(10) if i != original_label]\n                self.corrupted_labels[idx] = np.random.choice(wrong_labels)\n    \n    def __getitem__(self, idx):\n        image, label = self.base_dataset[idx]\n        \n        if idx in self.corrupted_labels:\n            label = self.corrupted_labels[idx]\n        \n        if isinstance(image, Image.Image):\n            image = transforms.ToTensor()(image)\n        \n        if self.feature_noise_std > 0:\n            noise = torch.randn_like(image) * self.feature_noise_std\n            image = torch.clamp(image + noise, 0, 1)\n        \n        if self.blur_prob > 0 and np.random.random() < self.blur_prob:\n            image = self._apply_blur(image)\n        \n        if self.contrast_factor < 1.0:\n            image = image * self.contrast_factor + 0.5 * (1 - self.contrast_factor)\n            image = torch.clamp(image, 0, 1)\n        \n        return image, label\n    \n    def _apply_blur(self, image):\n        img_np = image.permute(1, 2, 0).numpy()\n        img_np = (img_np * 255).astype(np.uint8)\n        blurred = cv2.GaussianBlur(img_np, (5, 5), 1.5)\n        return torch.from_numpy(blurred).permute(2, 0, 1).float() / 255.0\n    \n    def __len__(self):\n        return len(self.base_dataset)\n\ndef load_federated_cifar10(config):\n    \"\"\"Load CIFAR-10 with specific configuration\"\"\"\n    \n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n    \n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n    \n    train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                               download=True, transform=transform_train)\n    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                              download=True, transform=transform_test)\n    \n    # Create federated splits\n    num_classes = 10\n    class_indices = {i: [] for i in range(num_classes)}\n    for idx, (_, label) in enumerate(train_dataset):\n        class_indices[label].append(idx)\n    \n    client_indices = [[] for _ in range(config.num_clients)]\n    \n    for class_id in range(num_classes):\n        indices = class_indices[class_id]\n        np.random.shuffle(indices)\n        \n        proportions = np.random.dirichlet(np.repeat(config.alpha_dirichlet, config.num_clients))\n        proportions = np.maximum(proportions, 0.01)\n        proportions = proportions / proportions.sum()\n        \n        split_points = (np.cumsum(proportions) * len(indices)).astype(int)[:-1]\n        splits = np.split(indices, split_points)\n        \n        for client_id, split in enumerate(splits):\n            client_indices[client_id].extend(split)\n    \n    # Create client loaders with quality assignment\n    client_loaders = []\n    quality_assignments = []\n    \n    # Calculate client counts for each quality level\n    high_count = int(config.num_clients * config.high_quality_ratio)\n    medium_count = int(config.num_clients * config.medium_quality_ratio)\n    low_count = config.num_clients - high_count - medium_count\n    \n    for client_id, indices in enumerate(client_indices):\n        if len(indices) >= config.min_samples_per_client:\n            subset = Subset(train_dataset, indices)\n            \n            # Quality level assignment based on degradation configuration\n            if config.quality_level == 'low':  # Low degradation scenario\n                assigned_quality = 'low'  # All clients get high quality data\n            elif config.quality_level == 'medium':  # Medium degradation scenario\n                if client_id < high_count:\n                    assigned_quality = 'low'    # High quality data\n                elif client_id < high_count + medium_count:\n                    assigned_quality = 'medium' # Medium quality data\n                else:\n                    assigned_quality = 'high'   # Low quality data\n            else:  # 'high' degradation scenario\n                if client_id < high_count:\n                    assigned_quality = 'medium' # Medium quality data (best available)\n                elif client_id < high_count + medium_count:\n                    assigned_quality = 'high'   # Low quality data\n                else:\n                    assigned_quality = 'high'   # Low quality data (majority)\n            \n            quality_assignments.append(assigned_quality)\n            \n            if config.enable_quality_degradation:\n                degraded_dataset = QualityDegradedDataset(\n                    subset, quality_level=assigned_quality, corruption_seed=42 + client_id)\n                loader = DataLoader(degraded_dataset, batch_size=config.batch_size, \n                                  shuffle=True, num_workers=2)\n            else:\n                loader = DataLoader(subset, batch_size=config.batch_size, \n                                  shuffle=True, num_workers=2)\n            \n            client_loaders.append(loader)\n    \n    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n    \n    print(f\"📁 Data Ready: {len(client_loaders)} clients configured\")\n    return client_loaders, test_loader, quality_assignments\n\nclass FedAvgServer:\n    def __init__(self, config):\n        self.config = config\n        self.model = CIFAR10Model().to(config.device)\n        self.metrics = {'accuracy': [], 'loss': []}\n        \n    def aggregate(self, client_models, client_sizes):\n        total_size = sum(client_sizes)\n        weights = [size / total_size for size in client_sizes]\n        \n        global_dict = self.model.state_dict()\n        aggregated_dict = safe_aggregate_models(global_dict, client_models, weights)\n        \n        self.model.load_state_dict(aggregated_dict)\n        return copy.deepcopy(self.model)\n    \n    def evaluate(self, test_loader):\n        self.model.eval()\n        correct = 0\n        total = 0\n        test_loss = 0\n        \n        with torch.no_grad():\n            for data, target in test_loader:\n                data, target = data.to(self.config.device), target.to(self.config.device)\n                output = self.model(data)\n                test_loss += F.cross_entropy(output, target, reduction='sum').item()\n                pred = output.argmax(dim=1)\n                correct += pred.eq(target).sum().item()\n                total += target.size(0)\n        \n        accuracy = 100. * correct / total\n        loss = test_loss / total\n        self.metrics['accuracy'].append(accuracy)\n        self.metrics['loss'].append(loss)\n        return accuracy, loss\n\nclass FedProxServer:\n    def __init__(self, config):\n        self.config = config\n        self.model = CIFAR10Model().to(config.device)\n        self.metrics = {'accuracy': [], 'loss': []}\n        self.mu = 0.01  # Proximal term coefficient\n        \n    def aggregate(self, client_models, client_sizes):\n        total_size = sum(client_sizes)\n        weights = [size / total_size for size in client_sizes]\n        \n        global_dict = self.model.state_dict()\n        aggregated_dict = safe_aggregate_models(global_dict, client_models, weights)\n        \n        self.model.load_state_dict(aggregated_dict)\n        return copy.deepcopy(self.model)\n    \n    def evaluate(self, test_loader):\n        self.model.eval()\n        correct = 0\n        total = 0\n        test_loss = 0\n        \n        with torch.no_grad():\n            for data, target in test_loader:\n                data, target = data.to(self.config.device), target.to(self.config.device)\n                output = self.model(data)\n                test_loss += F.cross_entropy(output, target, reduction='sum').item()\n                pred = output.argmax(dim=1)\n                correct += pred.eq(target).sum().item()\n                total += target.size(0)\n        \n        accuracy = 100. * correct / total\n        loss = test_loss / total\n        self.metrics['accuracy'].append(accuracy)\n        self.metrics['loss'].append(loss)\n        return accuracy, loss\n\nclass FedNovaServer:\n    def __init__(self, config):\n        self.config = config\n        self.model = CIFAR10Model().to(config.device)\n        self.metrics = {'accuracy': [], 'loss': []}\n        \n    def aggregate(self, client_models, client_info):\n        \"\"\"FedNova aggregation with normalized averaging\"\"\"\n        total_data_size = sum(info[1] for info in client_info)\n        \n        effective_steps = []\n        weights = []\n        \n        for model, (local_steps, data_size) in zip(client_models, client_info):\n            effective_step = local_steps * (data_size / total_data_size)\n            effective_steps.append(effective_step)\n            weights.append(data_size / total_data_size)\n        \n        total_effective_steps = sum(effective_steps)\n        if total_effective_steps > 0:\n            normalized_weights = [step / total_effective_steps for step in effective_steps]\n        else:\n            normalized_weights = [1.0 / len(client_models)] * len(client_models)\n        \n        global_dict = self.model.state_dict()\n        aggregated_dict = safe_aggregate_models(global_dict, client_models, normalized_weights)\n        \n        self.model.load_state_dict(aggregated_dict)\n        return copy.deepcopy(self.model)\n    \n    def evaluate(self, test_loader):\n        self.model.eval()\n        correct = 0\n        total = 0\n        test_loss = 0\n        \n        with torch.no_grad():\n            for data, target in test_loader:\n                data, target = data.to(self.config.device), target.to(self.config.device)\n                output = self.model(data)\n                test_loss += F.cross_entropy(output, target, reduction='sum').item()\n                pred = output.argmax(dim=1)\n                correct += pred.eq(target).sum().item()\n                total += target.size(0)\n        \n        accuracy = 100. * correct / total\n        loss = test_loss / total\n        self.metrics['accuracy'].append(accuracy)\n        self.metrics['loss'].append(loss)\n        return accuracy, loss\n\nclass RobustSmartFedAvgServer:\n    def __init__(self, config):\n        self.config = config\n        self.model = CIFAR10Model().to(config.device)\n        self.metrics = {\n            'accuracy': [], 'loss': [], 'clients_filtered': [], \n            'avg_quality_score': [], 'filtering_effective': [],\n            'detected_quality_level': [], 'aggregation_strategy': []\n        }\n        \n        # Adaptive system parameters\n        self.round_number = 0\n        self.performance_history = []\n        self.quality_history = []\n        \n        print(f\"🧠 RobustSmartFedAvg initialized - Truly adaptive quality detection enabled\")\n    \n    def detect_global_quality_level(self, client_models, client_loaders):\n        \"\"\"Enhanced quality detection based on multiple indicators\"\"\"\n        if len(client_models) < 2:\n            return \"medium\"\n            \n        quality_metrics = []\n        loss_values = []\n        accuracy_values = []\n        \n        # Sample clients for quality assessment\n        sample_size = min(len(client_models), 5)\n        sample_indices = np.random.choice(len(client_models), sample_size, replace=False)\n        \n        for idx in sample_indices:\n            model, loader = client_models[idx], client_loaders[idx]\n            metrics = self._evaluate_client_quality(model, loader)\n            quality_metrics.append(metrics)\n            loss_values.append(metrics['loss'])\n            accuracy_values.append(metrics['accuracy'])\n        \n        # Calculate indicators\n        avg_accuracy = np.mean(accuracy_values)\n        avg_loss = np.mean(loss_values)\n        loss_variance = np.var(loss_values)\n        acc_variance = np.var(accuracy_values)\n        \n        # Enhanced quality classification with better thresholds\n        if self.round_number <= 2:\n            # Early rounds: Conservative detection based on actual CIFAR-10 performance\n            if avg_accuracy > 0.45 and avg_loss < 1.8:  # Good CIFAR-10 performance\n                detected_quality = \"high\"\n            elif avg_accuracy > 0.15 and avg_loss < 4.0:  # Moderate performance\n                detected_quality = \"medium\"\n            else:\n                detected_quality = \"low\"\n        else:\n            # Later rounds: Use historical performance and trends\n            if len(self.performance_history) >= 2:\n                recent_trend = self.performance_history[-1] - self.performance_history[-2]\n                current_performance = self.performance_history[-1]\n                \n                # Adjust thresholds based on current global performance\n                if current_performance > 60 and avg_accuracy > 0.40 and avg_loss < 2.0:\n                    detected_quality = \"high\"\n                elif current_performance > 25 and avg_accuracy > 0.12 and avg_loss < 5.0:\n                    detected_quality = \"medium\"\n                else:\n                    detected_quality = \"low\"\n            else:\n                # Fallback with better thresholds\n                if avg_accuracy > 0.35 and avg_loss < 2.5:\n                    detected_quality = \"high\"\n                elif avg_accuracy > 0.12 and avg_loss < 6.0:\n                    detected_quality = \"medium\"\n                else:\n                    detected_quality = \"low\"\n        \n        # Store quality history for adaptation\n        self.quality_history.append(detected_quality)\n        \n        return detected_quality\n    \n    def _evaluate_client_quality(self, client_model, client_loader):\n        \"\"\"Enhanced client quality evaluation\"\"\"\n        client_model.eval()\n        total_loss = 0.0\n        correct = 0\n        total = 0\n        sample_count = 0\n        loss_values = []\n        \n        with torch.no_grad():\n            for data, target in client_loader:\n                if sample_count >= 200:  # Adequate sampling\n                    break\n                \n                data, target = data.to(self.config.device), target.to(self.config.device)\n                output = client_model(data)\n                loss = F.cross_entropy(output, target)\n                total_loss += loss.item() * data.size(0)\n                loss_values.append(loss.item())\n                \n                pred = output.argmax(dim=1)\n                correct += pred.eq(target).sum().item()\n                total += data.size(0)\n                sample_count += data.size(0)\n        \n        return {\n            'accuracy': correct / max(total, 1),\n            'loss': total_loss / max(total, 1),\n            'loss_variance': np.var(loss_values) if loss_values else 0.0\n        }\n    \n    def adaptive_client_filtering(self, client_models, client_loaders, client_sizes, detected_quality):\n        \"\"\"Enhanced adaptive filtering based on detected quality and round history\"\"\"\n        \n        if detected_quality == \"high\":\n            return self._conservative_filtering(client_models, client_loaders, client_sizes)\n        elif detected_quality == \"medium\":\n            return self._balanced_filtering(client_models, client_loaders, client_sizes)\n        else:\n            return self._aggressive_filtering(client_models, client_loaders, client_sizes)\n    \n    def _conservative_filtering(self, client_models, client_loaders, client_sizes):\n        \"\"\"Ultra-conservative filtering for high-quality scenarios - behave exactly like FedAvg\"\"\"\n        stable_models, stable_loaders, stable_sizes = [], [], []\n        \n        for model, loader, size in zip(client_models, client_loaders, client_sizes):\n            # Only basic stability checks - no quality filtering in clean scenarios\n            stable, _ = self._basic_stability_check(model)\n            if stable:\n                stable_models.append(model)\n                stable_loaders.append(loader)\n                stable_sizes.append(size)\n        \n        # Always keep all stable clients in high quality scenarios\n        if len(stable_models) == 0:\n            stable_models, stable_loaders, stable_sizes = client_models, client_loaders, client_sizes\n        \n        filter_rate = 1.0 - (len(stable_models) / len(client_models))\n        # All clients get equal quality scores in high quality scenarios\n        quality_scores = [0.8] * len(stable_models)\n        \n        return stable_models, stable_loaders, stable_sizes, filter_rate, quality_scores\n    \n    def _balanced_filtering(self, client_models, client_loaders, client_sizes):\n        \"\"\"Balanced filtering for medium-quality scenarios\"\"\"\n        stable_models, stable_loaders, stable_sizes = [], [], []\n        quality_scores = []\n        \n        for model, loader, size in zip(client_models, client_loaders, client_sizes):\n            stable, _ = self._basic_stability_check(model)\n            if not stable:\n                continue\n                \n            quality_metrics = self._evaluate_client_quality(model, loader)\n            quality_score = self._compute_quality_score(quality_metrics, \"medium\")\n            \n            # Adaptive thresholds based on round number\n            acc_threshold = max(0.05, 0.15 - self.round_number * 0.01)\n            loss_threshold = min(10.0, 5.0 + self.round_number * 0.5)\n            \n            if quality_metrics['accuracy'] > acc_threshold and quality_metrics['loss'] < loss_threshold:\n                stable_models.append(model)\n                stable_loaders.append(loader)\n                stable_sizes.append(size)\n                quality_scores.append(quality_score)\n        \n        if len(stable_models) < 2:\n            return self._conservative_filtering(client_models, client_loaders, client_sizes)\n        \n        filter_rate = 1.0 - (len(stable_models) / len(client_models))\n        return stable_models, stable_loaders, stable_sizes, filter_rate, quality_scores\n    \n    def _aggressive_filtering(self, client_models, client_loaders, client_sizes):\n        \"\"\"Aggressive filtering for low-quality scenarios\"\"\"\n        stable_models, stable_loaders, stable_sizes = [], [], []\n        quality_metrics_list = []\n        \n        # Stage 1: Basic filtering\n        for model, loader, size in zip(client_models, client_loaders, client_sizes):\n            stable, _ = self._basic_stability_check(model)\n            if not stable:\n                continue\n                \n            quality_metrics = self._evaluate_client_quality(model, loader)\n            \n            # Very lenient thresholds for low quality scenarios\n            acc_threshold = max(0.03, 0.08 - self.round_number * 0.005)\n            loss_threshold = min(20.0, 12.0 + self.round_number * 0.8)\n            \n            if quality_metrics['accuracy'] > acc_threshold and quality_metrics['loss'] < loss_threshold:\n                stable_models.append(model)\n                stable_loaders.append(loader)\n                stable_sizes.append(size)\n                quality_metrics_list.append(quality_metrics)\n        \n        if len(stable_models) < 2:\n            return self._conservative_filtering(client_models, client_loaders, client_sizes)\n        \n        # Stage 2: Quality-based ranking and selection\n        quality_scores = [self._compute_quality_score(metrics, \"low\") \n                         for metrics in quality_metrics_list]\n        \n        # Keep top performers but ensure minimum participation\n        min_keep = max(2, len(stable_models) // 3)\n        if len(stable_models) > min_keep:\n            sorted_indices = np.argsort(quality_scores)[::-1]\n            keep_ratio = max(0.5, 0.8 - self.round_number * 0.05)  # Adaptive keep ratio\n            keep_count = max(min_keep, int(len(stable_models) * keep_ratio))\n            \n            final_models = [stable_models[i] for i in sorted_indices[:keep_count]]\n            final_loaders = [stable_loaders[i] for i in sorted_indices[:keep_count]]\n            final_sizes = [stable_sizes[i] for i in sorted_indices[:keep_count]]\n            final_quality_scores = [quality_scores[i] for i in sorted_indices[:keep_count]]\n        else:\n            final_models = stable_models\n            final_loaders = stable_loaders\n            final_sizes = stable_sizes\n            final_quality_scores = quality_scores\n        \n        filter_rate = 1.0 - (len(final_models) / len(client_models))\n        return final_models, final_loaders, final_sizes, filter_rate, final_quality_scores\n    \n    def _basic_stability_check(self, model):\n        \"\"\"Basic numerical stability check\"\"\"\n        for param in model.parameters():\n            if torch.isnan(param).any() or torch.isinf(param).any():\n                return False, \"NaN/Inf detected\"\n            if torch.abs(param).max() > 1000.0:\n                return False, \"Extreme values\"\n        return True, \"Stable\"\n    \n    def _compute_quality_score(self, metrics, quality_level):\n        \"\"\"Adaptive quality score computation\"\"\"\n        accuracy = metrics['accuracy']\n        loss = metrics['loss']\n        loss_var = metrics.get('loss_variance', 0.0)\n        \n        # Adaptive expectations based on detected quality level and round\n        if quality_level == \"high\":\n            expected_acc = max(0.5, 0.7 - self.round_number * 0.02)\n            expected_loss = min(2.0, 1.5 + self.round_number * 0.05)\n        elif quality_level == \"medium\":\n            expected_acc = max(0.15, 0.3 - self.round_number * 0.01)\n            expected_loss = min(6.0, 3.0 + self.round_number * 0.3)\n        else:  # low quality\n            expected_acc = max(0.08, 0.15 - self.round_number * 0.005)\n            expected_loss = min(15.0, 8.0 + self.round_number * 0.5)\n        \n        # Normalized scores with stability bonus\n        acc_score = min(accuracy / expected_acc, 1.0)\n        loss_score = max(0.0, 1.0 - loss / expected_loss)\n        stability_bonus = max(0.0, 1.0 - loss_var / 2.0)  # Reward stable clients\n        \n        return (acc_score + loss_score + 0.1 * stability_bonus) / 2.1\n    \n    def adaptive_aggregation(self, filtered_models, filtered_sizes, quality_scores, detected_quality):\n        \"\"\"Enhanced adaptive aggregation strategy\"\"\"\n        size_weights = np.array(filtered_sizes, dtype=float)\n        size_weights = size_weights / size_weights.sum()\n        \n        quality_weights = np.array(quality_scores)\n        if quality_weights.sum() > 0:\n            quality_weights = quality_weights / quality_weights.sum()\n        else:\n            quality_weights = size_weights.copy()\n        \n        # Adaptive weight combination based on quality and round history\n        if detected_quality == \"high\":\n            # High quality: Pure size-based aggregation (like FedAvg) for optimal performance\n            # Only use tiny quality influence if performance is declining\n            if len(self.performance_history) >= 2:\n                recent_decline = self.performance_history[-2] - self.performance_history[-1] > 2.0\n                alpha = 0.05 if recent_decline else 0.0  # Minimal quality influence only if needed\n            else:\n                alpha = 0.0  # Pure FedAvg in early rounds\n            combined_weights = (1-alpha) * size_weights + alpha * quality_weights\n            strategy = \"pure_size_based\" if alpha == 0.0 else \"size_based_rescue\"\n        elif detected_quality == \"medium\":\n            # Medium quality: Moderate quality influence with round adaptation\n            alpha = 0.25 + min(0.15, self.round_number * 0.015)\n            combined_weights = (1-alpha) * size_weights + alpha * quality_weights\n            strategy = \"balanced_adaptive\"\n        else:\n            # Low quality: Strong quality-focused aggregation\n            alpha = 0.65 + min(0.25, self.round_number * 0.02)\n            combined_weights = (1-alpha) * size_weights + alpha * quality_weights\n            strategy = \"quality_focused_adaptive\"\n        \n        self.metrics['aggregation_strategy'].append(strategy)\n        return combined_weights\n    \n    def aggregate(self, client_models, client_loaders, client_sizes):\n        \"\"\"Main aggregation method with enhanced adaptivity\"\"\"\n        self.round_number += 1\n        \n        # Enhanced quality detection\n        detected_quality = self.detect_global_quality_level(client_models, client_loaders)\n        self.metrics['detected_quality_level'].append(detected_quality)\n        \n        # Apply adaptive filtering\n        filtered_models, filtered_loaders, filtered_sizes, filter_rate, quality_scores = \\\n            self.adaptive_client_filtering(client_models, client_loaders, client_sizes, detected_quality)\n        \n        if not filtered_models:\n            return self._fallback_aggregate(client_models, client_sizes)\n        \n        # Apply adaptive aggregation\n        combined_weights = self.adaptive_aggregation(filtered_models, filtered_sizes, \n                                                   quality_scores, detected_quality)\n        \n        # Perform aggregation\n        global_dict = self.model.state_dict()\n        aggregated_dict = safe_aggregate_models(global_dict, filtered_models, combined_weights.tolist())\n        self.model.load_state_dict(aggregated_dict)\n        \n        # Store metrics as numbers only\n        self.metrics['clients_filtered'].append(float(filter_rate))\n        self.metrics['avg_quality_score'].append(float(np.mean(quality_scores)))\n        self.metrics['filtering_effective'].append(bool(filter_rate > 0.1))\n        \n        return copy.deepcopy(self.model)\n    \n    def _fallback_aggregate(self, client_models, client_sizes):\n        \"\"\"Fallback to simple FedAvg aggregation\"\"\"\n        total_size = sum(client_sizes)\n        weights = [size / total_size for size in client_sizes]\n        \n        global_dict = self.model.state_dict()\n        aggregated_dict = safe_aggregate_models(global_dict, client_models, weights)\n        self.model.load_state_dict(aggregated_dict)\n        \n        # Store fallback metrics\n        self.metrics['clients_filtered'].append(0.0)\n        self.metrics['avg_quality_score'].append(0.5)\n        self.metrics['filtering_effective'].append(False)\n        self.metrics['detected_quality_level'].append(\"unknown\")\n        self.metrics['aggregation_strategy'].append(\"fallback\")\n        \n        return copy.deepcopy(self.model)\n    \n    def evaluate(self, test_loader):\n        \"\"\"Standard evaluation with performance tracking\"\"\"\n        self.model.eval()\n        correct = 0\n        total = 0\n        test_loss = 0\n        \n        with torch.no_grad():\n            for data, target in test_loader:\n                data, target = data.to(self.config.device), target.to(self.config.device)\n                output = self.model(data)\n                test_loss += F.cross_entropy(output, target, reduction='sum').item()\n                pred = output.argmax(dim=1)\n                correct += pred.eq(target).sum().item()\n                total += target.size(0)\n        \n        accuracy = 100. * correct / total\n        loss = test_loss / total\n        self.metrics['accuracy'].append(accuracy)\n        self.metrics['loss'].append(loss)\n        \n        # Track performance for adaptation\n        self.performance_history.append(accuracy)\n        \n        return accuracy, loss\n\nclass Client:\n    def __init__(self, client_id, data_loader, config, is_fedprox=False):\n        self.client_id = client_id\n        self.data_loader = data_loader\n        self.config = config\n        self.is_fedprox = is_fedprox\n        self.mu = 0.01 if is_fedprox else 0.0\n        self.local_steps = 0\n        \n    def train(self, global_model):\n        model = copy.deepcopy(global_model)\n        model.train()\n        \n        # Adaptive learning rate based on quality level - only reduce for degraded data\n        adaptive_lr = self.config.lr\n        if self.config.quality_level == 'high':  # High degradation (low quality)\n            adaptive_lr *= 0.85  # Reduce LR for noisy data\n        elif self.config.quality_level == 'medium':  # Medium degradation\n            adaptive_lr *= 0.92  # Slight reduction\n        # No change for 'low' degradation (high quality) - use full learning rate\n        \n        optimizer = optim.SGD(model.parameters(), lr=adaptive_lr, \n                             momentum=0.9, weight_decay=1e-4)\n        \n        # Store global parameters for FedProx\n        if self.is_fedprox:\n            global_params = {name: param.clone() for name, param in global_model.named_parameters()}\n        \n        self.local_steps = 0\n        for epoch in range(self.config.local_epochs):\n            for data, target in self.data_loader:\n                data, target = data.to(self.config.device), target.to(self.config.device)\n                \n                optimizer.zero_grad()\n                output = model(data)\n                loss = F.cross_entropy(output, target)\n                \n                # Add proximal term for FedProx\n                if self.is_fedprox:\n                    prox_term = 0.0\n                    for name, param in model.named_parameters():\n                        prox_term += (self.mu / 2) * torch.norm(param - global_params[name]) ** 2\n                    loss += prox_term\n                \n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n                self.local_steps += 1\n        \n        return model\n    \n    def get_training_info(self):\n        return (self.local_steps, len(self.data_loader.dataset))\n\nclass FedProxClient(Client):\n    def __init__(self, client_id, data_loader, config):\n        super().__init__(client_id, data_loader, config, is_fedprox=True)\n\ndef format_time(seconds):\n    \"\"\"Format time in a readable way\"\"\"\n    if seconds < 60:\n        return f\"{seconds:.1f}s\"\n    elif seconds < 3600:\n        return f\"{seconds/60:.1f}m\"\n    else:\n        return f\"{seconds/3600:.1f}h\"\n\ndef get_non_iid_description(alpha):\n    \"\"\"Get human-readable description of non-IID level\"\"\"\n    if alpha <= 0.3:\n        return f\"{alpha} (High Non-IID)\"\n    elif alpha <= 0.5:\n        return f\"{alpha} (Medium Non-IID)\"\n    else:\n        return f\"{alpha} (Low Non-IID)\"\n\ndef get_quality_description(quality_level):\n    \"\"\"Get human-readable description of quality level\"\"\"\n    if quality_level == 'low':\n        return \"LOW DEGRADATION (High Quality)\"\n    elif quality_level == 'medium':\n        return \"MEDIUM DEGRADATION\"\n    else:\n        return \"HIGH DEGRADATION (Low Quality)\"\n\ndef run_single_method_experiment(method_name, server_class, client_class, config, \n                                client_loaders, test_loader, run_id):\n    \"\"\"Run experiment for a single method with detailed progress tracking\"\"\"\n    server = server_class(config)\n    clients = [client_class(i, loader, config) for i, loader in enumerate(client_loaders)]\n    client_sizes = [len(loader.dataset) for loader in client_loaders]\n    \n    results = []\n    round_times = []\n    \n    print(f\"      🔄 {method_name} Run {run_id} - Training on {config.device.type.upper()}\")\n    \n    for round_num in range(config.num_rounds):\n        round_start_time = time.time()\n        \n        # Client selection\n        selected_indices = np.random.choice(len(clients), config.clients_per_round, replace=False)\n        selected_clients = [clients[i] for i in selected_indices]\n        selected_loaders = [client_loaders[i] for i in selected_indices]\n        selected_sizes = [client_sizes[i] for i in selected_indices]\n        \n        # Local training\n        client_models = []\n        client_info = []\n        successful_loaders = []\n        successful_sizes = []\n        \n        for i, client in enumerate(selected_clients):\n            try:\n                model = client.train(server.model)\n                client_models.append(model)\n                client_info.append(client.get_training_info())\n                successful_loaders.append(selected_loaders[i])\n                successful_sizes.append(selected_sizes[i])\n            except Exception as e:\n                continue\n        \n        if not client_models:\n            continue\n        \n        # Aggregation\n        try:\n            if method_name == \"RobustSmartFedAvg\":\n                server.aggregate(client_models, successful_loaders, successful_sizes)\n            elif method_name == \"FedNova\":\n                server.aggregate(client_models, client_info)\n            else:  # FedAvg, FedProx\n                server.aggregate(client_models, successful_sizes)\n        except Exception as e:\n            print(f\"         Aggregation error: {e}\")\n            continue\n        \n        # Evaluation\n        try:\n            accuracy, loss = server.evaluate(test_loader)\n            round_time = time.time() - round_start_time\n            round_times.append(round_time)\n            \n            results.append({\n                'round': round_num + 1,\n                'accuracy': accuracy,\n                'loss': loss,\n                'time': round_time\n            })\n            \n            # Print progress every 2 rounds\n            if (round_num + 1) % 2 == 0:\n                debug_info = \"\"\n                if method_name == \"RobustSmartFedAvg\" and hasattr(server, 'metrics'):\n                    if server.metrics['clients_filtered']:\n                        filter_rate = server.metrics['clients_filtered'][-1] * 100\n                        detected_quality = server.metrics['detected_quality_level'][-1] if server.metrics['detected_quality_level'] else \"unknown\"\n                        strategy = server.metrics['aggregation_strategy'][-1] if server.metrics['aggregation_strategy'] else \"unknown\"\n                        debug_info = f\" [Quality: {detected_quality}, Strategy: {strategy}, Filtered: {filter_rate:.0f}%]\"\n                \n                print(f\"         Round {round_num + 1}: {accuracy:.2f}% acc, {loss:.3f} loss ({round_time:.1f}s){debug_info}\")\n                \n        except Exception as e:\n            print(f\"         Evaluation error: {e}\")\n            continue\n    \n    if results:\n        final_accuracy = results[-1]['accuracy']\n        best_accuracy = max(r['accuracy'] for r in results)\n        avg_time = np.mean(round_times) if round_times else 0\n    else:\n        final_accuracy = 0.0\n        best_accuracy = 0.0\n        avg_time = 0.0\n    \n    # Extended debug info for RobustSmartFedAvg\n    debug_suffix = \"\"\n    if method_name == \"RobustSmartFedAvg\" and hasattr(server, 'metrics') and server.metrics['clients_filtered']:\n        avg_filter_rate = np.mean(server.metrics['clients_filtered']) * 100\n        avg_quality = np.mean(server.metrics['avg_quality_score']) if server.metrics['avg_quality_score'] else 0\n        quality_levels = server.metrics['detected_quality_level']\n        most_common_quality = max(set(quality_levels), key=quality_levels.count) if quality_levels else \"unknown\"\n        debug_suffix = f\" [DetectedQuality: {most_common_quality}, AvgFilter: {avg_filter_rate:.0f}%, AvgQualityScore: {avg_quality:.2f}]\"\n    \n    print(f\"      ✅ {method_name} Run {run_id} Complete: Final={final_accuracy:.2f}%, Best={best_accuracy:.2f}% (avg {avg_time:.1f}s/round){debug_suffix}\")\n    \n    # Get method-specific metrics\n    method_metrics = {}\n    if hasattr(server, 'metrics'):\n        for key, values in server.metrics.items():\n            if key not in ['accuracy', 'loss'] and values:\n                try:\n                    if isinstance(values[0], (int, float, bool)):\n                        method_metrics[key] = float(np.mean([float(v) for v in values]))\n                    else:\n                        method_metrics[key] = str(values[-1])  # Store last value as string\n                except:\n                    method_metrics[key] = 0.0\n    \n    return {\n        'method': method_name,\n        'config': config.experiment_id,\n        'run_id': run_id,\n        'final_accuracy': final_accuracy,\n        'best_accuracy': best_accuracy,\n        'avg_time_per_round': avg_time,\n        'rounds': results,\n        'method_metrics': method_metrics\n    }\n\ndef save_parameter_set_results(lr, alpha, quality_level, set_number, method_results):\n    \"\"\"Save results for a single parameter set to JSON\"\"\"\n    results_data = {\n        'parameter_set': {\n            'set_number': set_number,\n            'learning_rate': lr,\n            'non_iid_alpha': alpha,\n            'quality_level': quality_level,\n            'timestamp': datetime.now().isoformat()\n        },\n        'methods': {}\n    }\n    \n    for method_name in ['FedAvg', 'FedProx', 'FedNova', 'RobustSmartFedAvg']:\n        method_data = [r for r in method_results if r['method'] == method_name]\n        if method_data:\n            final_accs = [r['final_accuracy'] for r in method_data if r['final_accuracy'] > 0]\n            best_accs = [r['best_accuracy'] for r in method_data if r['best_accuracy'] > 0]\n            avg_times = [r['avg_time_per_round'] for r in method_data if r['avg_time_per_round'] > 0]\n            \n            results_data['methods'][method_name] = {\n                'mean_final_accuracy': np.mean(final_accs) if final_accs else 0,\n                'std_final_accuracy': np.std(final_accs) if final_accs else 0,\n                'mean_best_accuracy': np.mean(best_accs) if best_accs else 0,\n                'mean_time_per_round': np.mean(avg_times) if avg_times else 0,\n                'successful_runs': len(final_accs),\n                'total_runs': len(method_data),\n                'individual_runs': method_data\n            }\n    \n    # Create filename\n    filename = f\"results_lr{lr}_alpha{alpha}_quality{quality_level}_set{set_number}.json\"\n    \n    # Save to file\n    with open(filename, 'w') as f:\n        json.dump(results_data, f, indent=2)\n    \n    return filename\n\ndef run_parameter_sweep():\n    \"\"\"Run comprehensive parameter sweep\"\"\"\n    print(\"=\" * 84)\n    print(\"🚀 ROBUST ADAPTIVE FEDERATED LEARNING PARAMETER SWEEP\")\n    print(\"💡 Testing Enhanced RobustSmartFedAvg vs FedAvg, FedProx, FedNova\")\n    print(\"🎯 Parameters: LR [0.01, 0.05, 0.09], α [0.3, 0.5, 0.7], Quality [low, medium, high]\")\n    print(\"🧠 RobustSmartFedAvg: Truly adaptive quality detection and dynamic strategy selection\")\n    print(\"=\" * 84)\n    \n    learning_rates = [0.01, 0.05, 0.09]\n    alpha_values = [0.3, 0.5, 0.7]\n    quality_levels = ['low', 'medium', 'high']\n    \n    methods = [\n        (\"FedAvg\", FedAvgServer, Client),\n        (\"FedProx\", FedProxServer, FedProxClient),\n        (\"FedNova\", FedNovaServer, Client),\n        (\"RobustSmartFedAvg\", RobustSmartFedAvgServer, Client),\n    ]\n    \n    total_combinations = len(learning_rates) * len(alpha_values) * len(quality_levels)\n    all_results = []\n    combination_count = 0\n    start_time = time.time()\n    \n    print(f\"\\n📊 PARAMETER SPACE OVERVIEW:\")\n    print(f\"   🎯 Learning Rates: {learning_rates}\")\n    print(f\"   🔄 Non-IID Levels (α): {alpha_values}\")\n    print(f\"   📉 Quality Levels: {quality_levels}\")\n    print(f\"   🔬 Total Combinations: {total_combinations}\")\n    print(f\"   🤖 Methods per Combination: {len(methods)}\")\n    print(f\"   📈 Total Experiments: {total_combinations * len(methods)}\")\n    \n    # Run parameter sweep\n    for lr, alpha, quality_level in product(learning_rates, alpha_values, quality_levels):\n        combination_count += 1\n        \n        print(f\"\\n{'='*84}\")\n        print(f\"📋 PARAMETER SET {combination_count}/{total_combinations}\")\n        print(f\"   🎯 Learning Rate: {lr}\")\n        print(f\"   🔄 Non-IID Level (α): {get_non_iid_description(alpha)}\")\n        print(f\"   📉 Quality Level: {get_quality_description(quality_level)}\")\n        print(f\"{'='*84}\")\n        \n        # Create configuration\n        config = ParameterSweepConfig(lr, alpha, quality_level)\n        \n        # Load data for this configuration\n        try:\n            client_loaders, test_loader, quality_assignments = load_federated_cifar10(config)\n        except Exception as e:\n            print(f\"❌ Data loading failed: {str(e)}\")\n            continue\n        \n        print(\"\")\n        \n        # Test each method\n        parameter_set_results = []\n        \n        for method_idx, (method_name, server_class, client_class) in enumerate(methods):\n            print(f\"🤖 METHOD {method_idx + 1}/4: {method_name}\")\n            print(f\"   📊 Training {config.num_rounds} rounds × {config.local_epochs} epochs\")\n            \n            method_results = []\n            \n            # Run experiment\n            for run_id in range(config.num_runs):\n                torch.manual_seed(config.seed_base + run_id)\n                np.random.seed(config.seed_base + run_id)\n                \n                try:\n                    result = run_single_method_experiment(\n                        method_name, server_class, client_class, config,\n                        client_loaders, test_loader, run_id + 1\n                    )\n                    method_results.append(result)\n                    parameter_set_results.append(result)\n                    \n                except Exception as e:\n                    print(f\"      ❌ Run {run_id + 1} failed: {str(e)}\")\n                    continue\n            \n            # Calculate method summary\n            if method_results:\n                final_accs = [r['final_accuracy'] for r in method_results if r['final_accuracy'] > 0]\n                if final_accs:\n                    avg_acc = np.mean(final_accs)\n                    std_acc = np.std(final_accs)\n                    success_rate = len(final_accs)\n                    total_runs = config.num_runs\n                    print(f\"   ✅ {method_name} Summary: {avg_acc:.2f}% ± {std_acc:.2f}% ({success_rate}/{total_runs} successful)\")\n                    \n                    # Store aggregated result\n                    all_results.append({\n                        'method': method_name,\n                        'lr': lr,\n                        'alpha': alpha,\n                        'quality_level': quality_level,\n                        'mean_accuracy': avg_acc,\n                        'std_accuracy': std_acc,\n                        'successful_runs': len(final_accs),\n                        'total_runs': config.num_runs,\n                        'individual_results': method_results\n                    })\n                else:\n                    print(f\"   ❌ {method_name} Summary: All runs failed\")\n            else:\n                print(f\"   ❌ {method_name} Summary: No successful runs\")\n            \n            print(\"\")\n        \n        # Save parameter set results\n        if parameter_set_results:\n            json_filename = save_parameter_set_results(lr, alpha, quality_level, combination_count, parameter_set_results)\n            \n            # Display parameter set summary\n            set_results = {}\n            for method_name in ['FedAvg', 'FedProx', 'FedNova', 'RobustSmartFedAvg']:\n                method_data = [r for r in parameter_set_results if r['method'] == method_name]\n                if method_data:\n                    final_accs = [r['final_accuracy'] for r in method_data if r['final_accuracy'] > 0]\n                    if final_accs:\n                        set_results[method_name] = np.mean(final_accs)\n            \n            print(\"📊 PARAMETER SET SUMMARY:\")\n            if set_results:\n                sorted_methods = sorted(set_results.items(), key=lambda x: x[1], reverse=True)\n                print(f\"   🏆 Best Method: {sorted_methods[0][0]} ({sorted_methods[0][1]:.2f}%)\")\n                for method, acc in sorted_methods:\n                    print(f\"   ✅ {method:18}: {acc:6.2f}%\")\n            \n            print(f\"   💾 Results saved: {json_filename}\")\n        \n        # Calculate and display progress\n        elapsed_time = time.time() - start_time\n        sets_remaining = total_combinations - combination_count\n        avg_time_per_set = elapsed_time / combination_count\n        estimated_remaining = avg_time_per_set * sets_remaining\n        \n        progress_pct = (combination_count / total_combinations) * 100\n        \n        print(f\"\\n⏱️  PROGRESS: {combination_count}/{total_combinations} sets complete ({progress_pct:.1f}%)\")\n        print(f\"   ⏱️  Elapsed: {format_time(elapsed_time)} | Remaining: ~{format_time(estimated_remaining)}\")\n    \n    return all_results\n\ndef analyze_results(all_results):\n    \"\"\"Analyze the parameter sweep results\"\"\"\n    print(f\"\\n{'='*80}\")\n    print(\"📊 ENHANCED ROBUSTSMARTFEDAVG ANALYSIS\")\n    print(f\"{'='*80}\")\n    \n    if not all_results:\n        print(\"❌ No results to analyze\")\n        return\n    \n    # Convert to DataFrame for analysis\n    df = pd.DataFrame(all_results)\n    \n    print(f\"📋 OVERALL PERFORMANCE SUMMARY:\")\n    print(\"-\" * 50)\n    \n    # Method ranking across all conditions\n    method_performance = df.groupby('method')['mean_accuracy'].agg(['mean', 'std', 'count']).round(2)\n    method_performance = method_performance.sort_values('mean', ascending=False)\n    \n    print(f\"🏆 Method Ranking (Overall Performance):\")\n    for i, (method, stats) in enumerate(method_performance.iterrows()):\n        emoji = \"🥇\" if i == 0 else \"🥈\" if i == 1 else \"🥉\" if i == 2 else \"📊\"\n        print(f\"   {i+1}. {emoji} {method:18}: {stats['mean']:5.2f}% ± {stats['std']:4.2f}% ({stats['count']:2d} configs)\")\n    \n    # Analyze RobustSmartFedAvg performance by condition\n    smart_results = df[df['method'] == 'RobustSmartFedAvg']\n    \n    if not smart_results.empty:\n        print(f\"\\n🧠 ROBUSTSMARTFEDAVG DETAILED ANALYSIS:\")\n        print(\"-\" * 50)\n        \n        # Performance by quality level\n        quality_performance = smart_results.groupby('quality_level')['mean_accuracy'].agg(['mean', 'std', 'count'])\n        print(f\"Performance by Quality Level:\")\n        for quality, stats in quality_performance.iterrows():\n            print(f\"   {quality:7}: {stats['mean']:5.2f}% ± {stats['std']:4.2f}% ({stats['count']:2d} configs)\")\n        \n        # Performance by learning rate\n        lr_performance = smart_results.groupby('lr')['mean_accuracy'].agg(['mean', 'std', 'count'])\n        print(f\"Performance by Learning Rate:\")\n        for lr, stats in lr_performance.iterrows():\n            print(f\"   LR={lr:4}: {stats['mean']:5.2f}% ± {stats['std']:4.2f}% ({stats['count']:2d} configs)\")\n        \n        # Performance by alpha\n        alpha_performance = smart_results.groupby('alpha')['mean_accuracy'].agg(['mean', 'std', 'count'])\n        print(f\"Performance by Non-IID Level:\")\n        for alpha, stats in alpha_performance.iterrows():\n            print(f\"   α={alpha:3}: {stats['mean']:5.2f}% ± {stats['std']:4.2f}% ({stats['count']:2d} configs)\")\n        \n        # Best configurations\n        best_configs = smart_results.nlargest(5, 'mean_accuracy')\n        print(f\"\\n🚀 Top 5 RobustSmartFedAvg Configurations:\")\n        for i, (_, row) in enumerate(best_configs.iterrows()):\n            print(f\"   {i+1}. {row['mean_accuracy']:.2f}% (LR={row['lr']}, α={row['alpha']}, Quality={row['quality_level']})\")\n    \n    # Compare with other methods by scenario\n    print(f\"\\n🔍 SCENARIO-BASED COMPARISON:\")\n    print(\"-\" * 50)\n    \n    scenarios = df.groupby(['quality_level', 'lr', 'alpha']).apply(\n        lambda x: x.loc[x['mean_accuracy'].idxmax()]\n    ).reset_index(drop=True)\n    \n    smart_wins = 0\n    total_scenarios = 0\n    \n    for _, scenario in scenarios.iterrows():\n        total_scenarios += 1\n        if scenario['method'] == 'RobustSmartFedAvg':\n            smart_wins += 1\n    \n    win_rate = (smart_wins / total_scenarios * 100) if total_scenarios > 0 else 0\n    print(f\"RobustSmartFedAvg Win Rate: {smart_wins}/{total_scenarios} scenarios ({win_rate:.1f}%)\")\n    \n    # Quality-specific analysis\n    for quality in ['low', 'medium', 'high']:\n        quality_scenarios = scenarios[scenarios['quality_level'] == quality]\n        quality_smart_wins = len(quality_scenarios[quality_scenarios['method'] == 'RobustSmartFedAvg'])\n        quality_total = len(quality_scenarios)\n        quality_win_rate = (quality_smart_wins / quality_total * 100) if quality_total > 0 else 0\n        \n        print(f\"   {quality:7} quality: {quality_smart_wins}/{quality_total} wins ({quality_win_rate:.1f}%)\")\n    \n    return df\n\ndef main():\n    \"\"\"Main function for enhanced parameter sweep\"\"\"\n    print(\"🚀 ENHANCED ROBUST ADAPTIVE FEDERATED LEARNING\")\n    print(\"💡 Testing Improved RobustSmartFedAvg vs FedAvg, FedProx, FedNova\")\n    print(\"🎯 Parameters: LR[0.01,0.05,0.09] × α[0.3,0.5,0.7] × Quality[low,medium,high]\")\n    print(\"🧠 RobustSmartFedAvg: Truly adaptive with enhanced quality detection\")\n    \n    try:\n        # Run parameter sweep\n        print(f\"\\n🔬 Starting enhanced parameter sweep...\")\n        all_results = run_parameter_sweep()\n        \n        if all_results:\n            # Analyze results\n            print(f\"\\n📊 Analyzing {len(all_results)} experiment results...\")\n            df = analyze_results(all_results)\n            \n            # Save overall results\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            csv_filename = f\"enhanced_robust_smartfedavg_sweep_{timestamp}.csv\"\n            df.to_csv(csv_filename, index=False)\n            print(f\"💾 Results saved to: {csv_filename}\")\n            \n            # Final summary\n            print(f\"\\n{'='*80}\")\n            print(\"🎉 ENHANCED ROBUST SMARTFEDAVG SWEEP COMPLETED!\")\n            print(f\"{'='*80}\")\n            \n            smart_results = df[df['method'] == 'RobustSmartFedAvg']\n            \n            if not smart_results.empty:\n                best_smart_acc = smart_results['mean_accuracy'].max()\n                avg_smart_acc = smart_results['mean_accuracy'].mean()\n                \n                print(f\"🏆 ENHANCED ROBUSTSMARTFEDAVG ACHIEVEMENTS:\")\n                print(f\"   🎯 Best Performance: {best_smart_acc:.2f}%\")\n                print(f\"   📊 Average Performance: {avg_smart_acc:.2f}%\")\n                print(f\"   🧠 Key Improvements:\")\n                print(f\"      • Enhanced adaptive quality detection\")\n                print(f\"      • Round-based strategy adaptation\")\n                print(f\"      • Dynamic threshold adjustment\")\n                print(f\"      • Improved stability and filtering\")\n                print(f\"   ✨ Now truly robust across all quality scenarios!\")\n        else:\n            print(\"❌ No results collected - check experiment setup\")\n        \n    except Exception as e:\n        print(f\"\\n❌ Enhanced parameter sweep failed: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T12:06:57.796229Z","iopub.execute_input":"2025-06-14T12:06:57.796937Z"}},"outputs":[{"name":"stdout","text":"🚀 ENHANCED ROBUST ADAPTIVE FEDERATED LEARNING\n💡 Testing Improved RobustSmartFedAvg vs FedAvg, FedProx, FedNova\n🎯 Parameters: LR[0.01,0.05,0.09] × α[0.3,0.5,0.7] × Quality[low,medium,high]\n🧠 RobustSmartFedAvg: Truly adaptive with enhanced quality detection\n\n🔬 Starting enhanced parameter sweep...\n====================================================================================\n🚀 ROBUST ADAPTIVE FEDERATED LEARNING PARAMETER SWEEP\n💡 Testing Enhanced RobustSmartFedAvg vs FedAvg, FedProx, FedNova\n🎯 Parameters: LR [0.01, 0.05, 0.09], α [0.3, 0.5, 0.7], Quality [low, medium, high]\n🧠 RobustSmartFedAvg: Truly adaptive quality detection and dynamic strategy selection\n====================================================================================\n\n📊 PARAMETER SPACE OVERVIEW:\n   🎯 Learning Rates: [0.01, 0.05, 0.09]\n   🔄 Non-IID Levels (α): [0.3, 0.5, 0.7]\n   📉 Quality Levels: ['low', 'medium', 'high']\n   🔬 Total Combinations: 27\n   🤖 Methods per Combination: 4\n   📈 Total Experiments: 108\n\n====================================================================================\n📋 PARAMETER SET 1/27\n   🎯 Learning Rate: 0.01\n   🔄 Non-IID Level (α): 0.3 (High Non-IID)\n   📉 Quality Level: LOW DEGRADATION (High Quality)\n====================================================================================\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170M/170M [00:02<00:00, 81.3MB/s] \n","output_type":"stream"},{"name":"stdout","text":"📁 Data Ready: 10 clients configured\n\n🤖 METHOD 1/4: FedAvg\n   📊 Training 8 rounds × 3 epochs\n      🔄 FedAvg Run 1 - Training on CUDA\n         Round 2: 46.95% acc, 1.401 loss (37.9s)\n         Round 4: 64.27% acc, 0.990 loss (38.6s)\n         Round 6: 66.56% acc, 0.933 loss (29.2s)\n         Round 8: 72.36% acc, 0.781 loss (33.6s)\n      ✅ FedAvg Run 1 Complete: Final=72.36%, Best=72.36% (avg 36.4s/round)\n   ✅ FedAvg Summary: 72.36% ± 0.00% (1/1 successful)\n\n🤖 METHOD 2/4: FedProx\n   📊 Training 8 rounds × 3 epochs\n      🔄 FedProx Run 1 - Training on CUDA\n         Round 2: 45.98% acc, 1.418 loss (58.7s)\n         Round 4: 63.95% acc, 0.993 loss (60.3s)\n         Round 6: 66.47% acc, 0.934 loss (47.3s)\n         Round 8: 71.79% acc, 0.795 loss (54.7s)\n      ✅ FedProx Run 1 Complete: Final=71.79%, Best=71.79% (avg 57.1s/round)\n   ✅ FedProx Summary: 71.79% ± 0.00% (1/1 successful)\n\n🤖 METHOD 3/4: FedNova\n   📊 Training 8 rounds × 3 epochs\n      🔄 FedNova Run 1 - Training on CUDA\n         Round 2: 50.01% acc, 1.379 loss (37.4s)\n         Round 4: 62.30% acc, 1.001 loss (39.1s)\n         Round 6: 66.47% acc, 0.944 loss (30.0s)\n         Round 8: 71.64% acc, 0.798 loss (35.4s)\n      ✅ FedNova Run 1 Complete: Final=71.64%, Best=71.66% (avg 36.7s/round)\n   ✅ FedNova Summary: 71.64% ± 0.00% (1/1 successful)\n\n🤖 METHOD 4/4: RobustSmartFedAvg\n   📊 Training 8 rounds × 3 epochs\n🧠 RobustSmartFedAvg initialized - Truly adaptive quality detection enabled\n      🔄 RobustSmartFedAvg Run 1 - Training on CUDA\n         Round 2: 45.13% acc, 1.404 loss (39.9s) [Quality: high, Strategy: pure_size_based, Filtered: 0%]\n         Round 4: 63.98% acc, 1.017 loss (39.4s) [Quality: medium, Strategy: balanced_adaptive, Filtered: 0%]\n         Round 6: 65.86% acc, 0.974 loss (34.4s) [Quality: high, Strategy: pure_size_based, Filtered: 0%]\n         Round 8: 72.08% acc, 0.796 loss (31.9s) [Quality: high, Strategy: pure_size_based, Filtered: 0%]\n      ✅ RobustSmartFedAvg Run 1 Complete: Final=72.08%, Best=72.08% (avg 38.3s/round) [DetectedQuality: high, AvgFilter: 0%, AvgQualityScore: 0.83]\n   ✅ RobustSmartFedAvg Summary: 72.08% ± 0.00% (1/1 successful)\n\n📊 PARAMETER SET SUMMARY:\n   🏆 Best Method: FedAvg (72.36%)\n   ✅ FedAvg            :  72.36%\n   ✅ RobustSmartFedAvg :  72.08%\n   ✅ FedProx           :  71.79%\n   ✅ FedNova           :  71.64%\n   💾 Results saved: results_lr0.01_alpha0.3_qualitylow_set1.json\n\n⏱️  PROGRESS: 1/27 sets complete (3.7%)\n   ⏱️  Elapsed: 22.9m | Remaining: ~9.9h\n\n====================================================================================\n📋 PARAMETER SET 2/27\n   🎯 Learning Rate: 0.01\n   🔄 Non-IID Level (α): 0.3 (High Non-IID)\n   📉 Quality Level: MEDIUM DEGRADATION\n====================================================================================\n📁 Data Ready: 10 clients configured\n\n🤖 METHOD 1/4: FedAvg\n   📊 Training 8 rounds × 3 epochs\n      🔄 FedAvg Run 1 - Training on CUDA\n         Round 2: 12.36% acc, 6.769 loss (45.2s)\n         Round 4: 12.47% acc, 14.404 loss (44.2s)\n         Round 6: 12.84% acc, 16.425 loss (55.4s)\n         Round 8: 18.73% acc, 15.636 loss (41.3s)\n      ✅ FedAvg Run 1 Complete: Final=18.73%, Best=18.73% (avg 47.8s/round)\n   ✅ FedAvg Summary: 18.73% ± 0.00% (1/1 successful)\n\n🤖 METHOD 2/4: FedProx\n   📊 Training 8 rounds × 3 epochs\n      🔄 FedProx Run 1 - Training on CUDA\n         Round 2: 12.96% acc, 6.459 loss (57.2s)\n         Round 4: 12.98% acc, 13.576 loss (56.1s)\n         Round 6: 12.36% acc, 15.741 loss (76.2s)\n         Round 8: 17.08% acc, 15.139 loss (58.4s)\n      ✅ FedProx Run 1 Complete: Final=17.08%, Best=17.08% (avg 62.6s/round)\n   ✅ FedProx Summary: 17.08% ± 0.00% (1/1 successful)\n\n🤖 METHOD 3/4: FedNova\n   📊 Training 8 rounds × 3 epochs\n      🔄 FedNova Run 1 - Training on CUDA\n         Round 2: 12.47% acc, 7.626 loss (44.7s)\n         Round 4: 13.57% acc, 14.577 loss (44.0s)\n         Round 6: 12.63% acc, 16.221 loss (56.1s)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}