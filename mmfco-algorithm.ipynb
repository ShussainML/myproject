{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fda3508c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-20T22:21:52.643672Z",
     "iopub.status.busy": "2025-06-20T22:21:52.643464Z",
     "iopub.status.idle": "2025-06-20T22:21:54.821791Z",
     "shell.execute_reply": "2025-06-20T22:21:54.821225Z"
    },
    "papermill": {
     "duration": 2.183501,
     "end_time": "2025-06-20T22:21:54.823192",
     "exception": false,
     "start_time": "2025-06-20T22:21:52.639691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a81c88ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T22:21:54.831074Z",
     "iopub.status.busy": "2025-06-20T22:21:54.830739Z",
     "iopub.status.idle": "2025-06-20T22:23:28.276251Z",
     "shell.execute_reply": "2025-06-20T22:23:28.275483Z"
    },
    "papermill": {
     "duration": 93.453612,
     "end_time": "2025-06-20T22:23:28.279579",
     "exception": false,
     "start_time": "2025-06-20T22:21:54.825967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Adaptive Multi-Modal Federated Learning Experiment\n",
      "============================================================\n",
      "⏰ Time Budget: 1.0 hours\n",
      "🧠 Features: Adaptive dataset discovery, intelligent scheduling, comprehensive results\n",
      "============================================================\n",
      "\n",
      "🎉 Experiment completed successfully!\n",
      "📊 Results saved to: /kaggle/working/adaptive_results\n",
      "📈 Visualizations saved to: /kaggle/working/plots\n",
      "🤖 Models saved to: /kaggle/working/models\n",
      "\n",
      "📋 Quick Summary:\n",
      "  sensor: 27.42% accuracy in 0.1 minutes\n",
      "  vision: 32.38% accuracy in 1.0 minutes\n",
      "\n",
      "✅ Adaptive Multi-Modal FL Experiment Complete!\n",
      "📁 Check the results directory for comprehensive outputs!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Adaptive Multi-Modal Federated Learning System 2025\n",
    "===================================================\n",
    "\n",
    "Intelligent system that:\n",
    "1. Discovers available datasets automatically\n",
    "2. Analyzes dataset characteristics and client resources\n",
    "3. Makes adaptive decisions about training priority and resource allocation\n",
    "4. Provides comprehensive results, visualizations, and adaptive strategies\n",
    "5. Handles clients with 2, 3, or 4 modalities intelligently\n",
    "\n",
    "Key Features:\n",
    "- Dataset Discovery Engine\n",
    "- Adaptive Training Scheduler  \n",
    "- Resource-Aware Model Selection\n",
    "- Comprehensive Results & Visualization\n",
    "- Priority-Based Training Strategy\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import psutil\n",
    "import logging\n",
    "import requests\n",
    "import zipfile\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple, Any, Optional, Union\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class DatasetCharacteristics:\n",
    "    \"\"\"Characteristics of a discovered dataset\"\"\"\n",
    "    modality: str\n",
    "    name: str\n",
    "    size: int\n",
    "    num_classes: int\n",
    "    complexity_score: float  # 0-1, higher = more complex\n",
    "    quality_score: float     # 0-1, higher = better quality\n",
    "    estimated_training_time: float  # minutes\n",
    "    memory_requirement: float       # GB\n",
    "    priority_score: float = field(init=False)  # 0-1, higher = train first (calculated)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # Calculate priority based on multiple factors\n",
    "        self.priority_score = (\n",
    "            0.4 * self.quality_score +\n",
    "            0.3 * (1.0 - self.complexity_score) +  # Easier datasets get higher priority\n",
    "            0.2 * min(self.size / 10000, 1.0) +    # Larger datasets get some priority\n",
    "            0.1 * (1.0 - self.memory_requirement / 8.0)  # Lower memory = higher priority\n",
    "        )\n",
    "\n",
    "@dataclass\n",
    "class ClientResources:\n",
    "    \"\"\"Client computational resources\"\"\"\n",
    "    available_memory_gb: float\n",
    "    cpu_cores: int\n",
    "    has_gpu: bool\n",
    "    gpu_memory_gb: float\n",
    "    estimated_time_budget_hours: float\n",
    "    \n",
    "    def get_resource_tier(self) -> str:\n",
    "        \"\"\"Classify client into resource tiers\"\"\"\n",
    "        if self.has_gpu and self.gpu_memory_gb > 8 and self.available_memory_gb > 16:\n",
    "            return \"high\"\n",
    "        elif self.has_gpu and self.gpu_memory_gb > 4 and self.available_memory_gb > 8:\n",
    "            return \"medium\"\n",
    "        else:\n",
    "            return \"low\"\n",
    "\n",
    "@dataclass\n",
    "class AdaptiveConfig:\n",
    "    \"\"\"Adaptive configuration that changes based on discoveries\"\"\"\n",
    "    # Base configuration\n",
    "    base_lr: float = 0.001\n",
    "    base_batch_size: int = 32\n",
    "    base_epochs: int = 10\n",
    "    \n",
    "    # Adaptive parameters (will be modified)\n",
    "    current_lr: float = field(init=False)\n",
    "    current_batch_size: int = field(init=False)\n",
    "    current_epochs: int = field(init=False)\n",
    "    \n",
    "    # Discovery settings\n",
    "    data_discovery_paths: List[str] = field(default_factory=lambda: [\n",
    "        \"/kaggle/working/datasets\",\n",
    "        \"/kaggle/input\",\n",
    "        \"./data\",\n",
    "        \"./datasets\"\n",
    "    ])\n",
    "    \n",
    "    # Results and visualization\n",
    "    save_detailed_results: bool = True\n",
    "    generate_visualizations: bool = True\n",
    "    save_adaptive_decisions: bool = True\n",
    "    \n",
    "    # Paths\n",
    "    results_dir: str = \"/kaggle/working/adaptive_results\"\n",
    "    plots_dir: str = \"/kaggle/working/plots\"\n",
    "    models_dir: str = \"/kaggle/working/models\"\n",
    "    logs_dir: str = \"/kaggle/working/logs\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # Initialize adaptive parameters\n",
    "        self.current_lr = self.base_lr\n",
    "        self.current_batch_size = self.base_batch_size\n",
    "        self.current_epochs = self.base_epochs\n",
    "        \n",
    "        # Create directories\n",
    "        for directory in [self.results_dir, self.plots_dir, self.models_dir, self.logs_dir]:\n",
    "            Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "class DatasetDiscoveryEngine:\n",
    "    \"\"\"Automatically discovers and analyzes available datasets\"\"\"\n",
    "    \n",
    "    def __init__(self, config: AdaptiveConfig):\n",
    "        self.config = config\n",
    "        self.discovered_datasets = {}\n",
    "        self.dataset_characteristics = {}\n",
    "        \n",
    "    def discover_datasets(self) -> Dict[str, DatasetCharacteristics]:\n",
    "        \"\"\"Discover available datasets and analyze their characteristics\"\"\"\n",
    "        logger.info(\"🔍 Starting dataset discovery...\")\n",
    "        \n",
    "        # For demo purposes, simulate different client scenarios\n",
    "        # In real implementation, this would scan actual directories\n",
    "        scenarios = self._generate_client_scenarios()\n",
    "        \n",
    "        # Randomly select a scenario (simulate different clients)\n",
    "        scenario_name = np.random.choice(list(scenarios.keys()))\n",
    "        selected_scenario = scenarios[scenario_name]\n",
    "        \n",
    "        logger.info(f\"📊 Simulating client scenario: '{scenario_name}'\")\n",
    "        logger.info(f\"🗂️  Available modalities: {list(selected_scenario.keys())}\")\n",
    "        \n",
    "        # Analyze each discovered dataset\n",
    "        for modality, dataset_info in selected_scenario.items():\n",
    "            characteristics = self._analyze_dataset_characteristics(modality, dataset_info)\n",
    "            self.dataset_characteristics[modality] = characteristics\n",
    "            \n",
    "            logger.info(f\"  📈 {modality}: Quality={characteristics.quality_score:.2f}, \"\n",
    "                       f\"Complexity={characteristics.complexity_score:.2f}, \"\n",
    "                       f\"Priority={characteristics.priority_score:.2f}\")\n",
    "        \n",
    "        # Save discovery results\n",
    "        self._save_discovery_results()\n",
    "        \n",
    "        return self.dataset_characteristics\n",
    "    \n",
    "    def _generate_client_scenarios(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Generate different client scenarios with varying dataset combinations\"\"\"\n",
    "        scenarios = {\n",
    "            \"medical_center\": {\n",
    "                \"vision\": {\"type\": \"medical_images\", \"size\": 8000, \"quality\": 0.9},\n",
    "                \"sensor\": {\"type\": \"patient_monitoring\", \"size\": 12000, \"quality\": 0.85},\n",
    "                \"text\": {\"type\": \"clinical_notes\", \"size\": 5000, \"quality\": 0.7}\n",
    "            },\n",
    "            \"smart_city\": {\n",
    "                \"vision\": {\"type\": \"traffic_cameras\", \"size\": 15000, \"quality\": 0.8},\n",
    "                \"sensor\": {\"type\": \"iot_sensors\", \"size\": 20000, \"quality\": 0.9},\n",
    "                \"multimodal\": {\"type\": \"surveillance\", \"size\": 3000, \"quality\": 0.7}\n",
    "            },\n",
    "            \"mobile_devices\": {\n",
    "                \"sensor\": {\"type\": \"smartphone_sensors\", \"size\": 25000, \"quality\": 0.95},\n",
    "                \"text\": {\"type\": \"user_messages\", \"size\": 18000, \"quality\": 0.6},\n",
    "                \"multimodal\": {\"type\": \"social_media\", \"size\": 8000, \"quality\": 0.75}\n",
    "            },\n",
    "            \"research_lab\": {\n",
    "                \"vision\": {\"type\": \"scientific_images\", \"size\": 12000, \"quality\": 0.95},\n",
    "                \"text\": {\"type\": \"research_papers\", \"size\": 10000, \"quality\": 0.9},\n",
    "                \"sensor\": {\"type\": \"lab_equipment\", \"size\": 8000, \"quality\": 0.8},\n",
    "                \"multimodal\": {\"type\": \"experiments\", \"size\": 4000, \"quality\": 0.85}\n",
    "            },\n",
    "            \"edge_device\": {\n",
    "                \"sensor\": {\"type\": \"environmental\", \"size\": 6000, \"quality\": 0.7},\n",
    "                \"vision\": {\"type\": \"security_camera\", \"size\": 4000, \"quality\": 0.6}\n",
    "            }\n",
    "        }\n",
    "        return scenarios\n",
    "    \n",
    "    def _analyze_dataset_characteristics(self, modality: str, dataset_info: Dict) -> DatasetCharacteristics:\n",
    "        \"\"\"Analyze characteristics of a discovered dataset\"\"\"\n",
    "        \n",
    "        # Extract basic info\n",
    "        dataset_type = dataset_info[\"type\"]\n",
    "        size = dataset_info[\"size\"]\n",
    "        quality = dataset_info[\"quality\"]\n",
    "        \n",
    "        # Determine complexity based on dataset type and modality\n",
    "        complexity_map = {\n",
    "            \"vision\": {\n",
    "                \"medical_images\": 0.9, \"traffic_cameras\": 0.6, \"scientific_images\": 0.95,\n",
    "                \"security_camera\": 0.4\n",
    "            },\n",
    "            \"text\": {\n",
    "                \"clinical_notes\": 0.8, \"user_messages\": 0.3, \"research_papers\": 0.9\n",
    "            },\n",
    "            \"sensor\": {\n",
    "                \"patient_monitoring\": 0.7, \"iot_sensors\": 0.5, \"smartphone_sensors\": 0.6,\n",
    "                \"lab_equipment\": 0.8, \"environmental\": 0.4\n",
    "            },\n",
    "            \"multimodal\": {\n",
    "                \"surveillance\": 0.85, \"social_media\": 0.7, \"experiments\": 0.9\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        complexity = complexity_map.get(modality, {}).get(dataset_type, 0.5)\n",
    "        \n",
    "        # Estimate training requirements\n",
    "        base_time_per_sample = {\"vision\": 0.01, \"text\": 0.005, \"sensor\": 0.003, \"multimodal\": 0.02}\n",
    "        estimated_time = (size * base_time_per_sample.get(modality, 0.01)) / 60  # minutes\n",
    "        \n",
    "        memory_map = {\"vision\": 2.0, \"text\": 1.0, \"sensor\": 0.5, \"multimodal\": 3.0}\n",
    "        memory_req = memory_map.get(modality, 1.0) * (size / 10000)\n",
    "        \n",
    "        # Determine number of classes (simplified)\n",
    "        num_classes_map = {\n",
    "            \"vision\": {\"medical_images\": 5, \"traffic_cameras\": 10, \"scientific_images\": 20, \"security_camera\": 3},\n",
    "            \"text\": {\"clinical_notes\": 8, \"user_messages\": 3, \"research_papers\": 15},\n",
    "            \"sensor\": {\"patient_monitoring\": 6, \"iot_sensors\": 4, \"smartphone_sensors\": 6, \"lab_equipment\": 8, \"environmental\": 4},\n",
    "            \"multimodal\": {\"surveillance\": 2, \"social_media\": 4, \"experiments\": 10}\n",
    "        }\n",
    "        \n",
    "        num_classes = num_classes_map.get(modality, {}).get(dataset_type, 5)\n",
    "        \n",
    "        return DatasetCharacteristics(\n",
    "            modality=modality,\n",
    "            name=dataset_type,\n",
    "            size=size,\n",
    "            num_classes=num_classes,\n",
    "            complexity_score=complexity,\n",
    "            quality_score=quality,\n",
    "            estimated_training_time=estimated_time,\n",
    "            memory_requirement=memory_req\n",
    "        )\n",
    "    \n",
    "    def _save_discovery_results(self):\n",
    "        \"\"\"Save dataset discovery results\"\"\"\n",
    "        discovery_results = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"discovered_datasets\": {\n",
    "                modality: {\n",
    "                    \"name\": char.name,\n",
    "                    \"size\": char.size,\n",
    "                    \"quality_score\": char.quality_score,\n",
    "                    \"complexity_score\": char.complexity_score,\n",
    "                    \"priority_score\": char.priority_score,\n",
    "                    \"estimated_training_time\": char.estimated_training_time,\n",
    "                    \"memory_requirement\": char.memory_requirement\n",
    "                }\n",
    "                for modality, char in self.dataset_characteristics.items()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save as JSON\n",
    "        with open(f\"{self.config.results_dir}/dataset_discovery.json\", \"w\") as f:\n",
    "            json.dump(discovery_results, f, indent=2)\n",
    "        \n",
    "        # Save as CSV for easy analysis\n",
    "        df_data = []\n",
    "        for modality, char in self.dataset_characteristics.items():\n",
    "            df_data.append({\n",
    "                \"modality\": modality,\n",
    "                \"name\": char.name,\n",
    "                \"size\": char.size,\n",
    "                \"quality\": char.quality_score,\n",
    "                \"complexity\": char.complexity_score,\n",
    "                \"priority\": char.priority_score,\n",
    "                \"est_time_min\": char.estimated_training_time,\n",
    "                \"memory_gb\": char.memory_requirement\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(df_data)\n",
    "        df.to_csv(f\"{self.config.results_dir}/dataset_analysis.csv\", index=False)\n",
    "        \n",
    "        logger.info(f\"💾 Discovery results saved to {self.config.results_dir}\")\n",
    "\n",
    "class AdaptiveTrainingScheduler:\n",
    "    \"\"\"Intelligent scheduler that adapts training based on resources and priorities\"\"\"\n",
    "    \n",
    "    def __init__(self, config: AdaptiveConfig, client_resources: ClientResources):\n",
    "        self.config = config\n",
    "        self.client_resources = client_resources\n",
    "        self.training_plan = []\n",
    "        self.adaptive_decisions = []\n",
    "        \n",
    "    def create_adaptive_training_plan(self, dataset_characteristics: Dict[str, DatasetCharacteristics]) -> List[Dict]:\n",
    "        \"\"\"Create an adaptive training plan based on resources and priorities\"\"\"\n",
    "        logger.info(\"🧠 Creating adaptive training plan...\")\n",
    "        \n",
    "        # Sort datasets by priority\n",
    "        sorted_datasets = sorted(\n",
    "            dataset_characteristics.items(),\n",
    "            key=lambda x: x[1].priority_score,\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        logger.info(\"📋 Dataset priority order:\")\n",
    "        for i, (modality, char) in enumerate(sorted_datasets):\n",
    "            logger.info(f\"  {i+1}. {modality} (priority: {char.priority_score:.3f})\")\n",
    "        \n",
    "        # Calculate resource allocation\n",
    "        total_time_budget = self.client_resources.estimated_time_budget_hours * 60  # minutes\n",
    "        total_memory_budget = self.client_resources.available_memory_gb\n",
    "        \n",
    "        # Adaptive resource allocation\n",
    "        training_plan = []\n",
    "        remaining_time = total_time_budget\n",
    "        remaining_memory = total_memory_budget\n",
    "        \n",
    "        for modality, characteristics in sorted_datasets:\n",
    "            # Check if we can fit this dataset\n",
    "            if (characteristics.estimated_training_time <= remaining_time * 1.2 and  # 20% buffer\n",
    "                characteristics.memory_requirement <= remaining_memory):\n",
    "                \n",
    "                # Adapt training parameters based on resources and dataset\n",
    "                adapted_params = self._adapt_training_parameters(characteristics)\n",
    "                \n",
    "                training_plan.append({\n",
    "                    \"modality\": modality,\n",
    "                    \"characteristics\": characteristics,\n",
    "                    \"adapted_params\": adapted_params,\n",
    "                    \"estimated_time\": characteristics.estimated_training_time,\n",
    "                    \"allocated_time\": min(characteristics.estimated_training_time * 1.5, remaining_time * 0.8)\n",
    "                })\n",
    "                \n",
    "                remaining_time -= characteristics.estimated_training_time\n",
    "                remaining_memory -= characteristics.memory_requirement * 0.5  # Assume 50% memory reuse\n",
    "                \n",
    "                decision = f\"✅ Including {modality}: Time={characteristics.estimated_training_time:.1f}min, Memory={characteristics.memory_requirement:.1f}GB\"\n",
    "                logger.info(f\"  {decision}\")\n",
    "                self.adaptive_decisions.append(decision)\n",
    "            else:\n",
    "                decision = f\"❌ Skipping {modality}: Insufficient resources (Time: {characteristics.estimated_training_time:.1f}/{remaining_time:.1f}min, Memory: {characteristics.memory_requirement:.1f}/{remaining_memory:.1f}GB)\"\n",
    "                logger.info(f\"  {decision}\")\n",
    "                self.adaptive_decisions.append(decision)\n",
    "        \n",
    "        self.training_plan = training_plan\n",
    "        self._save_training_plan()\n",
    "        \n",
    "        return training_plan\n",
    "    \n",
    "    def _adapt_training_parameters(self, characteristics: DatasetCharacteristics) -> Dict:\n",
    "        \"\"\"Adapt training parameters based on dataset characteristics and resources\"\"\"\n",
    "        \n",
    "        # Base parameters\n",
    "        adapted_params = {\n",
    "            \"epochs\": self.config.base_epochs,\n",
    "            \"batch_size\": self.config.base_batch_size,\n",
    "            \"learning_rate\": self.config.base_lr,\n",
    "            \"model_complexity\": \"medium\"\n",
    "        }\n",
    "        \n",
    "        # Adapt based on resource tier\n",
    "        resource_tier = self.client_resources.get_resource_tier()\n",
    "        \n",
    "        if resource_tier == \"high\":\n",
    "            adapted_params[\"batch_size\"] = min(128, self.config.base_batch_size * 4)\n",
    "            adapted_params[\"model_complexity\"] = \"high\"\n",
    "        elif resource_tier == \"low\":\n",
    "            adapted_params[\"batch_size\"] = max(8, self.config.base_batch_size // 2)\n",
    "            adapted_params[\"epochs\"] = max(5, self.config.base_epochs // 2)\n",
    "            adapted_params[\"model_complexity\"] = \"low\"\n",
    "        \n",
    "        # Adapt based on dataset characteristics\n",
    "        if characteristics.complexity_score > 0.8:\n",
    "            adapted_params[\"epochs\"] = int(adapted_params[\"epochs\"] * 1.5)\n",
    "            adapted_params[\"learning_rate\"] *= 0.5  # Lower LR for complex datasets\n",
    "        \n",
    "        if characteristics.quality_score < 0.6:\n",
    "            adapted_params[\"epochs\"] = max(5, int(adapted_params[\"epochs\"] * 0.7))  # Less training for low quality\n",
    "        \n",
    "        # Memory constraints\n",
    "        if characteristics.memory_requirement > self.client_resources.available_memory_gb * 0.5:\n",
    "            adapted_params[\"batch_size\"] = max(8, adapted_params[\"batch_size\"] // 2)\n",
    "            adapted_params[\"model_complexity\"] = \"low\"\n",
    "        \n",
    "        return adapted_params\n",
    "    \n",
    "    def _save_training_plan(self):\n",
    "        \"\"\"Save the adaptive training plan\"\"\"\n",
    "        plan_data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"client_resources\": {\n",
    "                \"memory_gb\": self.client_resources.available_memory_gb,\n",
    "                \"cpu_cores\": self.client_resources.cpu_cores,\n",
    "                \"has_gpu\": self.client_resources.has_gpu,\n",
    "                \"gpu_memory_gb\": self.client_resources.gpu_memory_gb,\n",
    "                \"time_budget_hours\": self.client_resources.estimated_time_budget_hours,\n",
    "                \"resource_tier\": self.client_resources.get_resource_tier()\n",
    "            },\n",
    "            \"training_plan\": [\n",
    "                {\n",
    "                    \"modality\": item[\"modality\"],\n",
    "                    \"priority_score\": item[\"characteristics\"].priority_score,\n",
    "                    \"dataset_size\": item[\"characteristics\"].size,\n",
    "                    \"adapted_epochs\": item[\"adapted_params\"][\"epochs\"],\n",
    "                    \"adapted_batch_size\": item[\"adapted_params\"][\"batch_size\"],\n",
    "                    \"adapted_lr\": item[\"adapted_params\"][\"learning_rate\"],\n",
    "                    \"model_complexity\": item[\"adapted_params\"][\"model_complexity\"],\n",
    "                    \"estimated_time_min\": item[\"estimated_time\"],\n",
    "                    \"allocated_time_min\": item[\"allocated_time\"]\n",
    "                }\n",
    "                for item in self.training_plan\n",
    "            ],\n",
    "            \"adaptive_decisions\": self.adaptive_decisions\n",
    "        }\n",
    "        \n",
    "        with open(f\"{self.config.results_dir}/adaptive_training_plan.json\", \"w\") as f:\n",
    "            json.dump(plan_data, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"📋 Training plan saved with {len(self.training_plan)} modalities\")\n",
    "\n",
    "class ComprehensiveResultsManager:\n",
    "    \"\"\"Manages comprehensive results, metrics, and visualizations\"\"\"\n",
    "    \n",
    "    def __init__(self, config: AdaptiveConfig):\n",
    "        self.config = config\n",
    "        self.training_results = {}\n",
    "        self.training_history = defaultdict(dict)\n",
    "        self.resource_usage = []\n",
    "        self.adaptive_decisions = []\n",
    "        \n",
    "    def log_training_start(self, modality: str, adapted_params: Dict):\n",
    "        \"\"\"Log the start of training for a modality\"\"\"\n",
    "        start_info = {\n",
    "            \"modality\": modality,\n",
    "            \"start_time\": datetime.now().isoformat(),\n",
    "            \"adapted_params\": adapted_params,\n",
    "            \"start_memory_gb\": psutil.virtual_memory().available / (1024**3),\n",
    "            \"start_cpu_percent\": psutil.cpu_percent(interval=1)\n",
    "        }\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            start_info[\"start_gpu_memory_gb\"] = torch.cuda.memory_allocated() / (1024**3)\n",
    "        \n",
    "        self.training_results[modality] = start_info\n",
    "        logger.info(f\"📊 Started training {modality} with {adapted_params}\")\n",
    "    \n",
    "    def log_epoch_results(self, modality: str, epoch: int, train_acc: float, val_acc: float, \n",
    "                         train_loss: float, val_loss: float, lr: float):\n",
    "        \"\"\"Log results for each epoch\"\"\"\n",
    "        if modality not in self.training_history:\n",
    "            self.training_history[modality] = {\n",
    "                \"epochs\": [], \"train_acc\": [], \"val_acc\": [],\n",
    "                \"train_loss\": [], \"val_loss\": [], \"learning_rates\": []\n",
    "            }\n",
    "        \n",
    "        self.training_history[modality][\"epochs\"].append(epoch)\n",
    "        self.training_history[modality][\"train_acc\"].append(train_acc)\n",
    "        self.training_history[modality][\"val_acc\"].append(val_acc)\n",
    "        self.training_history[modality][\"train_loss\"].append(train_loss)\n",
    "        self.training_history[modality][\"val_loss\"].append(val_loss)\n",
    "        self.training_history[modality][\"learning_rates\"].append(lr)\n",
    "    \n",
    "    def log_training_end(self, modality: str, final_results: Dict):\n",
    "        \"\"\"Log the end of training for a modality\"\"\"\n",
    "        end_info = {\n",
    "            \"end_time\": datetime.now().isoformat(),\n",
    "            \"final_train_accuracy\": final_results.get(\"final_train_accuracy\", 0),\n",
    "            \"best_val_accuracy\": final_results.get(\"best_val_accuracy\", 0),\n",
    "            \"epochs_completed\": final_results.get(\"epochs_trained\", 0),\n",
    "            \"end_memory_gb\": psutil.virtual_memory().available / (1024**3),\n",
    "            \"end_cpu_percent\": psutil.cpu_percent(interval=1)\n",
    "        }\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            end_info[\"end_gpu_memory_gb\"] = torch.cuda.memory_allocated() / (1024**3)\n",
    "        \n",
    "        # Calculate training duration\n",
    "        start_time = datetime.fromisoformat(self.training_results[modality][\"start_time\"])\n",
    "        end_time = datetime.fromisoformat(end_info[\"end_time\"])\n",
    "        duration_minutes = (end_time - start_time).total_seconds() / 60\n",
    "        end_info[\"training_duration_minutes\"] = duration_minutes\n",
    "        \n",
    "        self.training_results[modality].update(end_info)\n",
    "        \n",
    "        logger.info(f\"✅ Completed {modality}: {end_info['best_val_accuracy']:.2f}% accuracy in {duration_minutes:.1f} minutes\")\n",
    "    \n",
    "    def save_comprehensive_results(self):\n",
    "        \"\"\"Save all results and generate visualizations\"\"\"\n",
    "        logger.info(\"💾 Saving comprehensive results and generating visualizations...\")\n",
    "        \n",
    "        # Save detailed results\n",
    "        self._save_detailed_metrics()\n",
    "        \n",
    "        # Generate visualizations\n",
    "        if self.config.generate_visualizations:\n",
    "            self._generate_training_curves()\n",
    "            self._generate_performance_comparison()\n",
    "            self._generate_resource_usage_plots()\n",
    "            self._generate_adaptive_decisions_summary()\n",
    "        \n",
    "        # Save training history\n",
    "        self._save_training_history()\n",
    "        \n",
    "        # Generate final report\n",
    "        self._generate_final_report()\n",
    "        \n",
    "        logger.info(f\"📊 All results saved to {self.config.results_dir}\")\n",
    "        logger.info(f\"📈 Visualizations saved to {self.config.plots_dir}\")\n",
    "    \n",
    "    def _save_detailed_metrics(self):\n",
    "        \"\"\"Save detailed metrics for each modality\"\"\"\n",
    "        detailed_results = {\n",
    "            \"experiment_info\": {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"total_modalities_trained\": len(self.training_results),\n",
    "                \"total_experiment_time_minutes\": sum(\n",
    "                    result.get(\"training_duration_minutes\", 0) \n",
    "                    for result in self.training_results.values()\n",
    "                )\n",
    "            },\n",
    "            \"modality_results\": self.training_results,\n",
    "            \"training_history\": dict(self.training_history)\n",
    "        }\n",
    "        \n",
    "        # Save as JSON\n",
    "        with open(f\"{self.config.results_dir}/detailed_results.json\", \"w\") as f:\n",
    "            json.dump(detailed_results, f, indent=2, default=str)\n",
    "        \n",
    "        # Save as CSV for easy analysis\n",
    "        csv_data = []\n",
    "        for modality, results in self.training_results.items():\n",
    "            csv_data.append({\n",
    "                \"modality\": modality,\n",
    "                \"best_val_accuracy\": results.get(\"best_val_accuracy\", 0),\n",
    "                \"final_train_accuracy\": results.get(\"final_train_accuracy\", 0),\n",
    "                \"epochs_completed\": results.get(\"epochs_completed\", 0),\n",
    "                \"training_duration_min\": results.get(\"training_duration_minutes\", 0),\n",
    "                \"adapted_batch_size\": results.get(\"adapted_params\", {}).get(\"batch_size\", 0),\n",
    "                \"adapted_lr\": results.get(\"adapted_params\", {}).get(\"learning_rate\", 0),\n",
    "                \"model_complexity\": results.get(\"adapted_params\", {}).get(\"model_complexity\", \"\"),\n",
    "                \"memory_used_gb\": results.get(\"start_memory_gb\", 0) - results.get(\"end_memory_gb\", 0)\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(csv_data)\n",
    "        df.to_csv(f\"{self.config.results_dir}/results_summary.csv\", index=False)\n",
    "    \n",
    "    def _generate_training_curves(self):\n",
    "        \"\"\"Generate training curve plots for each modality\"\"\"\n",
    "        if not self.training_history:\n",
    "            return\n",
    "            \n",
    "        n_modalities = len(self.training_history)\n",
    "        fig, axes = plt.subplots(2, n_modalities, figsize=(5*n_modalities, 10))\n",
    "        \n",
    "        if n_modalities == 1:\n",
    "            axes = axes.reshape(2, 1)\n",
    "        \n",
    "        for idx, (modality, history) in enumerate(self.training_history.items()):\n",
    "            # Accuracy plot\n",
    "            axes[0, idx].plot(history[\"epochs\"], history[\"train_acc\"], 'b-', label='Train Accuracy', linewidth=2)\n",
    "            axes[0, idx].plot(history[\"epochs\"], history[\"val_acc\"], 'r-', label='Val Accuracy', linewidth=2)\n",
    "            axes[0, idx].set_title(f'{modality.title()} - Accuracy')\n",
    "            axes[0, idx].set_xlabel('Epoch')\n",
    "            axes[0, idx].set_ylabel('Accuracy (%)')\n",
    "            axes[0, idx].legend()\n",
    "            axes[0, idx].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Loss plot\n",
    "            axes[1, idx].plot(history[\"epochs\"], history[\"train_loss\"], 'b-', label='Train Loss', linewidth=2)\n",
    "            axes[1, idx].plot(history[\"epochs\"], history[\"val_loss\"], 'r-', label='Val Loss', linewidth=2)\n",
    "            axes[1, idx].set_title(f'{modality.title()} - Loss')\n",
    "            axes[1, idx].set_xlabel('Epoch')\n",
    "            axes[1, idx].set_ylabel('Loss')\n",
    "            axes[1, idx].legend()\n",
    "            axes[1, idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.config.plots_dir}/training_curves.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _generate_performance_comparison(self):\n",
    "        \"\"\"Generate performance comparison charts\"\"\"\n",
    "        if not self.training_results:\n",
    "            return\n",
    "            \n",
    "        modalities = list(self.training_results.keys())\n",
    "        accuracies = [self.training_results[mod].get(\"best_val_accuracy\", 0) for mod in modalities]\n",
    "        training_times = [self.training_results[mod].get(\"training_duration_minutes\", 0) for mod in modalities]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        bars1 = ax1.bar(modalities, accuracies, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "        ax1.set_title('Best Validation Accuracy by Modality', fontsize=14, fontweight='bold')\n",
    "        ax1.set_ylabel('Accuracy (%)')\n",
    "        ax1.set_ylim(0, 100)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, acc in zip(bars1, accuracies):\n",
    "            height = bar.get_height()\n",
    "            ax1.annotate(f'{acc:.1f}%', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                        xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "        \n",
    "        # Training time comparison  \n",
    "        bars2 = ax2.bar(modalities, training_times, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "        ax2.set_title('Training Duration by Modality', fontsize=14, fontweight='bold')\n",
    "        ax2.set_ylabel('Training Time (minutes)')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, time in zip(bars2, training_times):\n",
    "            height = bar.get_height()\n",
    "            ax2.annotate(f'{time:.1f}m', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                        xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.config.plots_dir}/performance_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _generate_resource_usage_plots(self):\n",
    "        \"\"\"Generate resource usage visualization\"\"\"\n",
    "        # Create a resource efficiency plot\n",
    "        if not self.training_results:\n",
    "            return\n",
    "            \n",
    "        modalities = list(self.training_results.keys())\n",
    "        accuracies = [self.training_results[mod].get(\"best_val_accuracy\", 0) for mod in modalities]\n",
    "        times = [self.training_results[mod].get(\"training_duration_minutes\", 1) for mod in modalities]\n",
    "        \n",
    "        # Calculate efficiency (accuracy per minute)\n",
    "        efficiency = [acc/time for acc, time in zip(accuracies, times)]\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "        \n",
    "        scatter = ax.scatter(times, accuracies, s=[e*10 for e in efficiency], \n",
    "                            c=efficiency, cmap='viridis', alpha=0.7)\n",
    "        \n",
    "        # Add labels for each point\n",
    "        for i, modality in enumerate(modalities):\n",
    "            ax.annotate(modality, (times[i], accuracies[i]), \n",
    "                       xytext=(5, 5), textcoords='offset points')\n",
    "        \n",
    "        ax.set_xlabel('Training Time (minutes)')\n",
    "        ax.set_ylabel('Best Validation Accuracy (%)')\n",
    "        ax.set_title('Training Efficiency: Accuracy vs Time\\n(Bubble size = Efficiency)')\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(scatter)\n",
    "        cbar.set_label('Efficiency (Accuracy/Minute)')\n",
    "        \n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.config.plots_dir}/resource_efficiency.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _generate_adaptive_decisions_summary(self):\n",
    "        \"\"\"Generate summary of adaptive decisions made\"\"\"\n",
    "        # Create a summary visualization of adaptive decisions\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        # Collect adaptive parameter information\n",
    "        modalities = []\n",
    "        complexities = []\n",
    "        batch_sizes = []\n",
    "        learning_rates = []\n",
    "        epochs = []\n",
    "        \n",
    "        for modality, results in self.training_results.items():\n",
    "            adapted_params = results.get(\"adapted_params\", {})\n",
    "            modalities.append(modality)\n",
    "            complexities.append(adapted_params.get(\"model_complexity\", \"medium\"))\n",
    "            batch_sizes.append(adapted_params.get(\"batch_size\", 32))\n",
    "            learning_rates.append(adapted_params.get(\"learning_rate\", 0.001))\n",
    "            epochs.append(adapted_params.get(\"epochs\", 10))\n",
    "        \n",
    "        # Create a table showing adaptive decisions\n",
    "        table_data = []\n",
    "        for i, mod in enumerate(modalities):\n",
    "            table_data.append([\n",
    "                mod.title(),\n",
    "                complexities[i],\n",
    "                f\"{batch_sizes[i]}\",\n",
    "                f\"{learning_rates[i]:.4f}\",\n",
    "                f\"{epochs[i]}\",\n",
    "                f\"{self.training_results[mod].get('best_val_accuracy', 0):.1f}%\"\n",
    "            ])\n",
    "        \n",
    "        # Create table\n",
    "        ax.axis('tight')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        table = ax.table(cellText=table_data,\n",
    "                        colLabels=['Modality', 'Model Complexity', 'Batch Size', 'Learning Rate', 'Epochs', 'Best Accuracy'],\n",
    "                        cellLoc='center',\n",
    "                        loc='center')\n",
    "        \n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.scale(1.2, 1.5)\n",
    "        \n",
    "        # Style the table\n",
    "        for i in range(len(table_data) + 1):\n",
    "            for j in range(6):\n",
    "                if i == 0:  # Header row\n",
    "                    table[(i, j)].set_facecolor('#4ECDC4')\n",
    "                    table[(i, j)].set_text_props(weight='bold')\n",
    "                else:\n",
    "                    table[(i, j)].set_facecolor('#F8F9FA' if i % 2 == 0 else 'white')\n",
    "        \n",
    "        ax.set_title('Adaptive Training Decisions Summary', fontsize=16, fontweight='bold', pad=20)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.config.plots_dir}/adaptive_decisions.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _save_training_history(self):\n",
    "        \"\"\"Save training history in multiple formats\"\"\"\n",
    "        # Save as pickle for easy loading in Python\n",
    "        with open(f\"{self.config.results_dir}/training_history.pkl\", \"wb\") as f:\n",
    "            pickle.dump(dict(self.training_history), f)\n",
    "        \n",
    "        # Save as CSV for each modality\n",
    "        for modality, history in self.training_history.items():\n",
    "            df = pd.DataFrame(history)\n",
    "            df.to_csv(f\"{self.config.results_dir}/history_{modality}.csv\", index=False)\n",
    "    \n",
    "    def _generate_final_report(self):\n",
    "        \"\"\"Generate a comprehensive final report\"\"\"\n",
    "        report = []\n",
    "        report.append(\"# Adaptive Multi-Modal Federated Learning - Final Report\")\n",
    "        report.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Executive Summary\n",
    "        report.append(\"## Executive Summary\")\n",
    "        total_modalities = len(self.training_results)\n",
    "        avg_accuracy = np.mean([r.get(\"best_val_accuracy\", 0) for r in self.training_results.values()])\n",
    "        total_time = sum([r.get(\"training_duration_minutes\", 0) for r in self.training_results.values()])\n",
    "        \n",
    "        report.append(f\"- **Total Modalities Trained**: {total_modalities}\")\n",
    "        report.append(f\"- **Average Best Accuracy**: {avg_accuracy:.2f}%\")\n",
    "        report.append(f\"- **Total Training Time**: {total_time:.1f} minutes\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Individual Results\n",
    "        report.append(\"## Individual Modality Results\")\n",
    "        for modality, results in self.training_results.items():\n",
    "            report.append(f\"### {modality.title()}\")\n",
    "            report.append(f\"- **Best Validation Accuracy**: {results.get('best_val_accuracy', 0):.2f}%\")\n",
    "            report.append(f\"- **Training Duration**: {results.get('training_duration_minutes', 0):.1f} minutes\")\n",
    "            report.append(f\"- **Epochs Completed**: {results.get('epochs_completed', 0)}\")\n",
    "            \n",
    "            adapted_params = results.get('adapted_params', {})\n",
    "            report.append(f\"- **Adapted Parameters**:\")\n",
    "            report.append(f\"  - Model Complexity: {adapted_params.get('model_complexity', 'N/A')}\")\n",
    "            report.append(f\"  - Batch Size: {adapted_params.get('batch_size', 'N/A')}\")\n",
    "            report.append(f\"  - Learning Rate: {adapted_params.get('learning_rate', 'N/A')}\")\n",
    "            report.append(\"\")\n",
    "        \n",
    "        # Adaptive Insights\n",
    "        report.append(\"## Adaptive System Insights\")\n",
    "        if total_modalities > 1:\n",
    "            best_modality = max(self.training_results.items(), key=lambda x: x[1].get(\"best_val_accuracy\", 0))\n",
    "            fastest_modality = min(self.training_results.items(), key=lambda x: x[1].get(\"training_duration_minutes\", float('inf')))\n",
    "            \n",
    "            report.append(f\"- **Best Performing Modality**: {best_modality[0]} ({best_modality[1].get('best_val_accuracy', 0):.2f}%)\")\n",
    "            report.append(f\"- **Fastest Training Modality**: {fastest_modality[0]} ({fastest_modality[1].get('training_duration_minutes', 0):.1f} minutes)\")\n",
    "        \n",
    "        report.append(\"\")\n",
    "        report.append(\"## Files Generated\")\n",
    "        report.append(\"- `detailed_results.json`: Complete results in JSON format\")\n",
    "        report.append(\"- `results_summary.csv`: Summary results in CSV format\")\n",
    "        report.append(\"- `training_curves.png`: Training accuracy and loss curves\")\n",
    "        report.append(\"- `performance_comparison.png`: Performance comparison charts\")\n",
    "        report.append(\"- `resource_efficiency.png`: Resource efficiency analysis\")\n",
    "        report.append(\"- `adaptive_decisions.png`: Summary of adaptive decisions\")\n",
    "        \n",
    "        # Save report\n",
    "        with open(f\"{self.config.results_dir}/final_report.md\", \"w\") as f:\n",
    "            f.write(\"\\n\".join(report))\n",
    "\n",
    "# Simple dataset classes for demo (using the same ones from before but simplified)\n",
    "class SimpleVisionDataset(Dataset):\n",
    "    def __init__(self, size: int, num_classes: int):\n",
    "        self.data = [(torch.randn(3, 224, 224), i % num_classes) for i in range(size)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.data[idx]\n",
    "        return {'data': data, 'label': label, 'modality': 'vision'}\n",
    "\n",
    "class SimpleTextDataset(Dataset):\n",
    "    def __init__(self, size: int, num_classes: int):\n",
    "        self.data = [(torch.randint(0, 1000, (50,)), i % num_classes) for i in range(size)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.data[idx]\n",
    "        return {'data': data, 'label': label, 'modality': 'text'}\n",
    "\n",
    "class SimpleSensorDataset(Dataset):\n",
    "    def __init__(self, size: int, num_classes: int):\n",
    "        self.data = [(torch.randn(6, 128), i % num_classes) for i in range(size)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.data[idx]\n",
    "        return {'data': data, 'label': label, 'modality': 'sensor'}\n",
    "\n",
    "class SimpleMultiModalDataset(Dataset):\n",
    "    def __init__(self, size: int, num_classes: int):\n",
    "        self.data = [({\n",
    "            'image': torch.randn(3, 224, 224),\n",
    "            'text': torch.randint(0, 1000, (20,))\n",
    "        }, i % num_classes) for i in range(size)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.data[idx]\n",
    "        return {'data': data, 'label': label, 'modality': 'multimodal'}\n",
    "\n",
    "# Simple model classes\n",
    "class SimpleVisionModel(nn.Module):\n",
    "    def __init__(self, num_classes: int, complexity: str = \"medium\"):\n",
    "        super().__init__()\n",
    "        if complexity == \"low\":\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, 7, stride=4), nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d(4), nn.Flatten(),\n",
    "                nn.Linear(32*16, num_classes)\n",
    "            )\n",
    "        else:\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, 7, stride=2), nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, 5, stride=2), nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d(4), nn.Flatten(),\n",
    "                nn.Linear(128*16, 256), nn.ReLU(),\n",
    "                nn.Linear(256, num_classes)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "class SimpleTextModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, num_classes: int, complexity: str = \"medium\"):\n",
    "        super().__init__()\n",
    "        embed_dim = 64 if complexity == \"low\" else 128\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, embed_dim, batch_first=True)\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        return self.classifier(hidden[-1])\n",
    "\n",
    "class SimpleSensorModel(nn.Module):\n",
    "    def __init__(self, num_classes: int, complexity: str = \"medium\"):\n",
    "        super().__init__()\n",
    "        channels = 32 if complexity == \"low\" else 64\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(6, channels, 7), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1), nn.Flatten(),\n",
    "            nn.Linear(channels, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "class SimpleMultiModalModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, num_classes: int, complexity: str = \"medium\"):\n",
    "        super().__init__()\n",
    "        embed_dim = 64 if complexity == \"low\" else 128\n",
    "        \n",
    "        self.vision_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 7, stride=4), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "            nn.Linear(32, embed_dim)\n",
    "        )\n",
    "        \n",
    "        self.text_encoder = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embed_dim),\n",
    "            nn.LSTM(embed_dim, embed_dim, batch_first=True)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(embed_dim * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        image_features = self.vision_encoder(x['image'])\n",
    "        \n",
    "        text_embedded = self.text_encoder[0](x['text'])\n",
    "        _, (text_hidden, _) = self.text_encoder[1](text_embedded)\n",
    "        text_features = text_hidden[-1]\n",
    "        \n",
    "        fused = torch.cat([image_features, text_features], dim=1)\n",
    "        return self.classifier(fused)\n",
    "\n",
    "# Main Adaptive System\n",
    "class AdaptiveMultiModalFL:\n",
    "    \"\"\"Main adaptive multi-modal federated learning system\"\"\"\n",
    "    \n",
    "    def __init__(self, time_budget_hours: float = 2.0):\n",
    "        # Initialize configuration\n",
    "        self.config = AdaptiveConfig()\n",
    "        \n",
    "        # Simulate client resources (in real system, would detect automatically)\n",
    "        memory = psutil.virtual_memory()\n",
    "        self.client_resources = ClientResources(\n",
    "            available_memory_gb=memory.available / (1024**3),\n",
    "            cpu_cores=psutil.cpu_count(),\n",
    "            has_gpu=torch.cuda.is_available(),\n",
    "            gpu_memory_gb=torch.cuda.get_device_properties(0).total_memory / (1024**3) if torch.cuda.is_available() else 0,\n",
    "            estimated_time_budget_hours=time_budget_hours\n",
    "        )\n",
    "        \n",
    "        # Initialize components\n",
    "        self.discovery_engine = DatasetDiscoveryEngine(self.config)\n",
    "        self.scheduler = AdaptiveTrainingScheduler(self.config, self.client_resources)\n",
    "        self.results_manager = ComprehensiveResultsManager(self.config)\n",
    "        \n",
    "        # Data and models\n",
    "        self.datasets = {}\n",
    "        self.models = {}\n",
    "        \n",
    "        logger.info(\"🚀 Initialized Adaptive Multi-Modal FL System\")\n",
    "        logger.info(f\"💻 Client Resources: {self.client_resources.get_resource_tier()} tier\")\n",
    "        logger.info(f\"⏰ Time Budget: {time_budget_hours} hours\")\n",
    "    \n",
    "    def run_adaptive_experiment(self):\n",
    "        \"\"\"Run the complete adaptive experiment\"\"\"\n",
    "        logger.info(\"🔬 Starting Adaptive Multi-Modal FL Experiment...\")\n",
    "        \n",
    "        # Step 1: Discover available datasets\n",
    "        dataset_characteristics = self.discovery_engine.discover_datasets()\n",
    "        \n",
    "        # Step 2: Create adaptive training plan\n",
    "        training_plan = self.scheduler.create_adaptive_training_plan(dataset_characteristics)\n",
    "        \n",
    "        if not training_plan:\n",
    "            logger.error(\"❌ No datasets can be trained with current resources!\")\n",
    "            return None\n",
    "        \n",
    "        # Step 3: Load datasets and initialize models\n",
    "        self._load_datasets_and_models(training_plan)\n",
    "        \n",
    "        # Step 4: Execute adaptive training\n",
    "        self._execute_adaptive_training(training_plan)\n",
    "        \n",
    "        # Step 5: Save comprehensive results\n",
    "        self.results_manager.save_comprehensive_results()\n",
    "        \n",
    "        logger.info(\"🎉 Adaptive experiment completed successfully!\")\n",
    "        return self.results_manager.training_results\n",
    "    \n",
    "    def _load_datasets_and_models(self, training_plan: List[Dict]):\n",
    "        \"\"\"Load datasets and initialize models based on training plan\"\"\"\n",
    "        logger.info(\"📚 Loading datasets and initializing models...\")\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        for plan_item in training_plan:\n",
    "            modality = plan_item[\"modality\"]\n",
    "            characteristics = plan_item[\"characteristics\"]\n",
    "            adapted_params = plan_item[\"adapted_params\"]\n",
    "            \n",
    "            # Create dataset (simplified for demo)\n",
    "            if modality == \"vision\":\n",
    "                dataset = SimpleVisionDataset(characteristics.size, characteristics.num_classes)\n",
    "                model = SimpleVisionModel(characteristics.num_classes, adapted_params[\"model_complexity\"])\n",
    "            elif modality == \"text\":\n",
    "                dataset = SimpleTextDataset(characteristics.size, characteristics.num_classes)\n",
    "                model = SimpleTextModel(1000, characteristics.num_classes, adapted_params[\"model_complexity\"])\n",
    "            elif modality == \"sensor\":\n",
    "                dataset = SimpleSensorDataset(characteristics.size, characteristics.num_classes)\n",
    "                model = SimpleSensorModel(characteristics.num_classes, adapted_params[\"model_complexity\"])\n",
    "            elif modality == \"multimodal\":\n",
    "                dataset = SimpleMultiModalDataset(characteristics.size, characteristics.num_classes)\n",
    "                model = SimpleMultiModalModel(1000, characteristics.num_classes, adapted_params[\"model_complexity\"])\n",
    "            \n",
    "            # Split dataset\n",
    "            train_size = int(0.8 * len(dataset))\n",
    "            val_size = len(dataset) - train_size\n",
    "            train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "            \n",
    "            # Create dataloaders\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=adapted_params[\"batch_size\"],\n",
    "                shuffle=True\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=adapted_params[\"batch_size\"],\n",
    "                shuffle=False\n",
    "            )\n",
    "            \n",
    "            self.datasets[modality] = {\"train\": train_loader, \"val\": val_loader}\n",
    "            self.models[modality] = model.to(device)\n",
    "            \n",
    "            logger.info(f\"  ✅ {modality}: {len(train_dataset)} train, {len(val_dataset)} val samples\")\n",
    "    \n",
    "    def _execute_adaptive_training(self, training_plan: List[Dict]):\n",
    "        \"\"\"Execute training according to the adaptive plan\"\"\"\n",
    "        logger.info(\"🎯 Executing adaptive training...\")\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        for plan_item in training_plan:\n",
    "            modality = plan_item[\"modality\"]\n",
    "            adapted_params = plan_item[\"adapted_params\"]\n",
    "            \n",
    "            logger.info(f\"\\n🔥 Training {modality} with adaptive parameters...\")\n",
    "            \n",
    "            # Log training start\n",
    "            self.results_manager.log_training_start(modality, adapted_params)\n",
    "            \n",
    "            # Get model and data\n",
    "            model = self.models[modality]\n",
    "            train_loader = self.datasets[modality][\"train\"]\n",
    "            val_loader = self.datasets[modality][\"val\"]\n",
    "            \n",
    "            # Setup training\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                model.parameters(),\n",
    "                lr=adapted_params[\"learning_rate\"],\n",
    "                weight_decay=1e-4\n",
    "            )\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            \n",
    "            best_val_acc = 0.0\n",
    "            \n",
    "            # Training loop\n",
    "            for epoch in range(adapted_params[\"epochs\"]):\n",
    "                # Training phase\n",
    "                model.train()\n",
    "                train_loss = 0.0\n",
    "                train_correct = 0\n",
    "                train_total = 0\n",
    "                \n",
    "                for batch in train_loader:\n",
    "                    if isinstance(batch['data'], dict):\n",
    "                        data = {k: v.to(device) for k, v in batch['data'].items()}\n",
    "                    else:\n",
    "                        data = batch['data'].to(device)\n",
    "                    \n",
    "                    labels = batch['label'].to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(data)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    train_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    train_total += labels.size(0)\n",
    "                    train_correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                # Validation phase\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for batch in val_loader:\n",
    "                        if isinstance(batch['data'], dict):\n",
    "                            data = {k: v.to(device) for k, v in batch['data'].items()}\n",
    "                        else:\n",
    "                            data = batch['data'].to(device)\n",
    "                        \n",
    "                        labels = batch['label'].to(device)\n",
    "                        outputs = model(data)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        \n",
    "                        val_loss += loss.item()\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        val_total += labels.size(0)\n",
    "                        val_correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                # Calculate metrics\n",
    "                train_acc = 100. * train_correct / train_total\n",
    "                val_acc = 100. * val_correct / val_total\n",
    "                avg_train_loss = train_loss / len(train_loader)\n",
    "                avg_val_loss = val_loss / len(val_loader)\n",
    "                \n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    # Save best model\n",
    "                    torch.save(model.state_dict(), f\"{self.config.models_dir}/{modality}_best.pth\")\n",
    "                \n",
    "                # Log epoch results\n",
    "                self.results_manager.log_epoch_results(\n",
    "                    modality, epoch + 1, train_acc, val_acc, \n",
    "                    avg_train_loss, avg_val_loss, adapted_params[\"learning_rate\"]\n",
    "                )\n",
    "                \n",
    "                logger.info(f\"  Epoch {epoch+1}/{adapted_params['epochs']}: \"\n",
    "                           f\"Train {train_acc:.2f}%, Val {val_acc:.2f}%\")\n",
    "            \n",
    "            # Log training end\n",
    "            final_results = {\n",
    "                \"best_val_accuracy\": best_val_acc,\n",
    "                \"final_train_accuracy\": train_acc,\n",
    "                \"epochs_trained\": adapted_params[\"epochs\"]\n",
    "            }\n",
    "            self.results_manager.log_training_end(modality, final_results)\n",
    "\n",
    "def run_adaptive_fl_experiment(time_budget_hours: float = 1.0):\n",
    "    \"\"\"Run the adaptive FL experiment with specified time budget\"\"\"\n",
    "    print(\"🚀 Adaptive Multi-Modal Federated Learning Experiment\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"⏰ Time Budget: {time_budget_hours} hours\")\n",
    "    print(\"🧠 Features: Adaptive dataset discovery, intelligent scheduling, comprehensive results\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Initialize system\n",
    "        system = AdaptiveMultiModalFL(time_budget_hours=time_budget_hours)\n",
    "        \n",
    "        # Run experiment\n",
    "        results = system.run_adaptive_experiment()\n",
    "        \n",
    "        if results:\n",
    "            print(f\"\\n🎉 Experiment completed successfully!\")\n",
    "            print(f\"📊 Results saved to: {system.config.results_dir}\")\n",
    "            print(f\"📈 Visualizations saved to: {system.config.plots_dir}\")\n",
    "            print(f\"🤖 Models saved to: {system.config.models_dir}\")\n",
    "            \n",
    "            # Print quick summary\n",
    "            print(f\"\\n📋 Quick Summary:\")\n",
    "            for modality, result in results.items():\n",
    "                acc = result.get(\"best_val_accuracy\", 0)\n",
    "                time = result.get(\"training_duration_minutes\", 0)\n",
    "                print(f\"  {modality}: {acc:.2f}% accuracy in {time:.1f} minutes\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Experiment failed: {e}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run adaptive experiment with 1 hour time budget\n",
    "    results = run_adaptive_fl_experiment(time_budget_hours=1.0)\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\n✅ Adaptive Multi-Modal FL Experiment Complete!\")\n",
    "        print(\"📁 Check the results directory for comprehensive outputs!\")\n",
    "    else:\n",
    "        print(\"\\n❌ Experiment failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6e5af",
   "metadata": {
    "papermill": {
     "duration": 0.002156,
     "end_time": "2025-06-20T22:23:28.284112",
     "exception": false,
     "start_time": "2025-06-20T22:23:28.281956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 105.25936,
   "end_time": "2025-06-20T22:23:31.446122",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-20T22:21:46.186762",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
